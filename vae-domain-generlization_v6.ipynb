{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T17:28:21.248292Z","iopub.status.busy":"2024-10-04T17:28:21.247484Z","iopub.status.idle":"2024-10-04T17:28:21.254250Z","shell.execute_reply":"2024-10-04T17:28:21.253246Z","shell.execute_reply.started":"2024-10-04T17:28:21.248250Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms, models\n","from tqdm import tqdm\n","from PIL import Image\n","import os\n","import torch.nn.functional as F\n","from torch.nn.modules.loss import _WeightedLoss"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T17:28:21.256575Z","iopub.status.busy":"2024-10-04T17:28:21.256200Z","iopub.status.idle":"2024-10-04T17:28:21.265992Z","shell.execute_reply":"2024-10-04T17:28:21.265126Z","shell.execute_reply.started":"2024-10-04T17:28:21.256523Z"},"trusted":true},"outputs":[],"source":["import random\n","import numpy as np\n","\n","def set_random_seeds(seed_value=42):\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n","    np.random.seed(seed_value)  # Numpy module.\n","    random.seed(seed_value)  # Python random module.\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","set_random_seeds()"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T17:28:21.267387Z","iopub.status.busy":"2024-10-04T17:28:21.267024Z","iopub.status.idle":"2024-10-04T17:28:21.285519Z","shell.execute_reply":"2024-10-04T17:28:21.284496Z","shell.execute_reply.started":"2024-10-04T17:28:21.267346Z"},"trusted":true},"outputs":[],"source":["class PACSDataset(Dataset):\n","    def __init__(self, root_dir, domain, transform=None):\n","        self.root_dir = root_dir\n","        self.domain = domain\n","        self.transform = transform\n","        self.images, self.labels = self._load_images_labels()\n","\n","    def _load_images_labels(self):\n","        image_paths = []\n","        labels = []\n","        domain_dir = os.path.join(self.root_dir, self.domain)\n","        classes = sorted(\n","            [\n","                d\n","                for d in os.listdir(domain_dir)\n","                if os.path.isdir(os.path.join(domain_dir, d))\n","            ]\n","        )\n","\n","        for label, class_name in enumerate(classes):\n","            class_dir = os.path.join(domain_dir, class_name)\n","            for image_name in os.listdir(class_dir):\n","                if image_name.endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    image_paths.append(os.path.join(class_dir, image_name))\n","                    labels.append(label)\n","\n","        return image_paths, labels\n","\n","    def __len__(self):\n","        return len(self.images)  # Return the number of images\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path).convert(\"RGB\")\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","\n","def get_transform():\n","    return transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(10),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0),\n","    ])\n","\n","\n","def get_dataloaders(root_dir, domain, batch_size=32):\n","    train_paths, train_labels, val_paths, val_labels, test_paths, test_labels = (\n","        split_dataset(root_dir, domain)\n","    )\n","\n","    train_dataset = PACSDatasetFromPaths(\n","        train_paths, train_labels, transform=get_transform()\n","    )\n","    val_dataset = PACSDatasetFromPaths(val_paths, val_labels, transform=get_transform())\n","    test_dataset = PACSDatasetFromPaths(\n","        test_paths, test_labels, transform=get_transform()\n","    )\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader\n","\n","\n","def get_mixed_dataloader(root_dir, domains, batch_size=32):\n","    datasets = [\n","        PACSDataset(root_dir, domain, transform=get_transform()) for domain in domains\n","    ]\n","    combined_dataset = torch.utils.data.ConcatDataset(datasets)\n","    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T17:28:21.288262Z","iopub.status.busy":"2024-10-04T17:28:21.287443Z","iopub.status.idle":"2024-10-04T17:28:21.310421Z","shell.execute_reply":"2024-10-04T17:28:21.309446Z","shell.execute_reply.started":"2024-10-04T17:28:21.288217Z"},"trusted":true},"outputs":[],"source":["# Define Encoder, Decoder, Classifier\n","class Encoder(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(Encoder, self).__init__()\n","\n","        # Sử dụng EfficientNet-B1\n","        self.efficientnet = models.efficientnet_b1(pretrained=True)\n","\n","        # Freeze EfficientNet layers\n","        for param in self.efficientnet.parameters():\n","            param.requires_grad = False\n","\n","        # Lấy số features từ lớp cuối cùng của EfficientNet-B1\n","        in_features = self.efficientnet.classifier[1].in_features\n","\n","        # Attention mechanism\n","        self.attention = nn.Sequential(\n","            nn.Linear(in_features, in_features // 16),\n","            nn.ReLU(),\n","            nn.Linear(in_features // 16, in_features),\n","            nn.Sigmoid(),\n","        )\n","\n","        # Mean (mu) and log-variance (logvar) layers\n","        self.fc_mu = nn.Linear(in_features, latent_dim)\n","        self.fc_logvar = nn.Linear(in_features, latent_dim)\n","\n","    def forward(self, x):\n","        # Pass input through EfficientNet feature extractor\n","        features = self.efficientnet.features(x)\n","        x = self.efficientnet.avgpool(features)\n","        x = torch.flatten(x, 1)\n","\n","        # Apply attention\n","        attention_weights = self.attention(x)\n","        x = x * attention_weights\n","\n","        # Compute mu and logvar\n","        mu = self.fc_mu(x)\n","        logvar = self.fc_logvar(x)\n","        return mu, logvar\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.shortcut = nn.Sequential()\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","    def forward(self, x):\n","        residual = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(residual)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, latent_dim, num_domains):\n","        super(Decoder, self).__init__()\n","\n","        self.domain_embedding = nn.Embedding(num_domains, latent_dim)\n","\n","        self.fc = nn.Linear(latent_dim, 512 * 7 * 7)\n","\n","        self.decoder = nn.Sequential(\n","            ResidualBlock(512, 256),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            ResidualBlock(128, 128),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            ResidualBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh()  # Thêm Tanh để đảm bảo đầu ra trong khoảng [-1, 1]\n","        )\n","\n","        # Attention mechanism\n","        self.attention = nn.Sequential(nn.Conv2d(3, 1, kernel_size=1), nn.Sigmoid())\n","\n","    def forward(self, z, domain_label):\n","        domain_embed = self.domain_embedding(domain_label)\n","        z = z + domain_embed\n","\n","        x = self.fc(z)\n","        x = x.view(-1, 512, 7, 7)\n","        x = self.decoder(x)\n","\n","        # Apply attention\n","        attention_map = self.attention(x)\n","        x = x * attention_map\n","\n","        return x\n","\n","\n","class Classifier(nn.Module):\n","    def __init__(self, latent_dim, num_classes):\n","        super(Classifier, self).__init__()\n","        self.fc = nn.Linear(latent_dim, num_classes)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, z):\n","        z = self.dropout(z)\n","        return self.fc(z)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T17:28:21.455143Z","iopub.status.busy":"2024-10-04T17:28:21.454332Z","iopub.status.idle":"2024-10-04T17:28:21.475670Z","shell.execute_reply":"2024-10-04T17:28:21.474787Z","shell.execute_reply.started":"2024-10-04T17:28:21.455098Z"},"trusted":true},"outputs":[],"source":["def reparameterize(mu, logvar):\n","    std = torch.exp(0.5 * logvar)\n","    eps = torch.randn_like(std)\n","    return mu + eps * std\n","\n","\n","class LabelSmoothingLoss(_WeightedLoss):\n","    def __init__(self, weight=None, reduction=\"mean\", smoothing=0.0):\n","        super().__init__(weight=weight, reduction=reduction)\n","        self.smoothing = smoothing\n","        self.weight = weight\n","        self.reduction = reduction\n","\n","    def k_one_hot(self, targets: torch.Tensor, n_classes: int, smoothing=0.0):\n","        with torch.no_grad():\n","            targets = (\n","                torch.empty(size=(targets.size(0), n_classes), device=targets.device)\n","                .fill_(smoothing / (n_classes - 1))\n","                .scatter_(1, targets.data.unsqueeze(1), 1.0 - smoothing)\n","            )\n","        return targets\n","\n","    def reduce_loss(self, loss):\n","        return (\n","            loss.mean()\n","            if self.reduction == \"mean\"\n","            else loss.sum() if self.reduction == \"sum\" else loss\n","        )\n","\n","    def forward(self, inputs, targets):\n","        assert 0 <= self.smoothing < 1\n","\n","        targets = self.k_one_hot(targets, inputs.size(-1), self.smoothing)\n","        log_preds = F.log_softmax(inputs, -1)\n","\n","        if self.weight is not None:\n","            log_preds = log_preds * self.weight.unsqueeze(0)\n","\n","        return self.reduce_loss(-(targets * log_preds).sum(dim=-1))\n","\n","class DynamicWeightBalancer:\n","    def __init__(self, init_alpha=0.5, init_beta=2.0, init_gamma=0.1, patience=5, scaling_factor=0.8):\n","        self.alpha = init_alpha  # Reconstruction loss weight\n","        self.beta = init_beta    # Classification loss weight\n","        self.gamma = init_gamma  # KL divergence weight\n","        self.patience = patience\n","        self.scaling_factor = scaling_factor\n","        self.best_loss = float('inf')\n","        self.counter = 0\n","\n","    def update(self, current_loss, recon_loss, clf_loss, kl_loss):\n","        if current_loss < self.best_loss:\n","            self.best_loss = current_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","\n","        if self.counter >= self.patience:\n","            self.counter = 0\n","            # Increase classification weight and decrease others\n","            self.beta /= self.scaling_factor\n","            self.alpha *= self.scaling_factor\n","            self.gamma *= self.scaling_factor\n","\n","        # Ensure classification loss weight is always significantly larger\n","        total_weight = self.alpha + self.beta + self.gamma\n","        self.alpha = max(0.1, min(0.3, self.alpha / total_weight))\n","        self.beta = max(0.6, min(0.8, self.beta / total_weight))\n","        self.gamma = 1 - self.alpha - self.beta\n","\n","        return self.alpha, self.beta, self.gamma\n","\n","def compute_loss(reconstructed_imgs_list, original_imgs, mu, logvar, predicted_labels, true_labels, clf_loss_fn, epoch, total_epochs, balancer):\n","    recon_loss = sum(\n","        F.mse_loss(recon, original_imgs, reduction=\"mean\")\n","        for recon in reconstructed_imgs_list\n","    ) / len(reconstructed_imgs_list)\n","\n","    kld_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n","    clf_loss = clf_loss_fn(predicted_labels, true_labels)\n","\n","    alpha, beta, gamma = balancer.update(recon_loss + clf_loss + kld_loss, recon_loss, clf_loss, kld_loss)\n","\n","    total_loss = alpha * recon_loss + beta * clf_loss + gamma * kld_loss\n","    return total_loss, recon_loss.item(), clf_loss.item(), kld_loss.item(), alpha, beta, gamma"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T17:28:21.478654Z","iopub.status.busy":"2024-10-04T17:28:21.478285Z","iopub.status.idle":"2024-10-04T17:28:21.498515Z","shell.execute_reply":"2024-10-04T17:28:21.497669Z","shell.execute_reply.started":"2024-10-04T17:28:21.478613Z"},"trusted":true},"outputs":[],"source":["def mixup_data(x, y, alpha=1.0, device=\"cuda\"):\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(device)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n","\n","class LossWeightScheduler:\n","    def __init__(self, init_alpha=0.1, init_beta=1.0, init_gamma=0.1, decay_factor=0.9, decay_epochs=10):\n","        self.alpha = init_alpha\n","        self.beta = init_beta\n","        self.gamma = init_gamma\n","        self.decay_factor = decay_factor\n","        self.decay_epochs = decay_epochs\n","\n","    def step(self, epoch):\n","        if (epoch + 1) % self.decay_epochs == 0:\n","            self.alpha *= self.decay_factor\n","            self.gamma *= self.decay_factor\n","        return self.alpha, self.beta, self.gamma\n","\n","\n","def train_model_progressive(\n","    encoder,\n","    decoders,\n","    classifier,\n","    domains,\n","    dataloader,\n","    val_loaders,\n","    optimizer,\n","    scheduler,\n","    num_epochs=100,\n","    device=\"cuda\",\n","    patience=10,\n","):\n","    clf_loss_fn = LabelSmoothingLoss(smoothing=0.1)\n","    domain_to_idx = {domain: idx for idx, domain in enumerate(domains)}\n","\n","    best_loss = float(\"inf\")\n","    patience_counter = 0\n","\n","    balancer = DynamicWeightBalancer()\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","        encoder.train()\n","        classifier.train()\n","        for decoder in decoders.values():\n","            decoder.train()\n","\n","        running_loss = 0.0\n","        running_recon_loss = 0.0\n","        running_clf_loss = 0.0\n","        running_kl_loss = 0.0\n","\n","        for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            inputs, labels_a, labels_b, lam = mixup_data(\n","                inputs, labels, alpha=0.2, device=device\n","            )\n","\n","            mu, logvar = encoder(inputs)\n","            z = reparameterize(mu, logvar)\n","\n","            reconstructed_imgs_list = []\n","            for domain in domains:\n","                domain_label = torch.tensor(\n","                    [domain_to_idx[domain]] * inputs.size(0), device=device\n","                )\n","                reconstructed_imgs = decoders[domain](z, domain_label)\n","                reconstructed_imgs_list.append(reconstructed_imgs)\n","\n","            predicted_labels = classifier(z)\n","\n","            loss, recon_loss, clf_loss, kl_loss, alpha, beta, gamma = compute_loss(\n","                reconstructed_imgs_list,\n","                inputs,\n","                mu,\n","                logvar,\n","                predicted_labels,\n","                labels,\n","                lambda pred, target: mixup_criterion(\n","                    clf_loss_fn, pred, labels_a, labels_b, lam\n","                ),\n","                epoch,\n","                num_epochs,\n","                balancer,\n","            )\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            running_recon_loss += recon_loss\n","            running_clf_loss += clf_loss\n","            running_kl_loss += kl_loss\n","\n","        avg_loss = running_loss / len(dataloader)\n","        avg_recon_loss = running_recon_loss / len(dataloader)\n","        avg_clf_loss = running_clf_loss / len(dataloader)\n","        avg_kl_loss = running_kl_loss / len(dataloader)\n","\n","        print(\n","            f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Recon: {avg_recon_loss:.4f}, Clf: {avg_clf_loss:.4f}, KL: {avg_kl_loss:.4f}\"\n","        )\n","        print(f\"Weights - Alpha: {alpha:.4f}, Beta: {beta:.4f}, Gamma: {gamma:.4f}\")\n","\n","        # Evaluate on all domains\n","        encoder.eval()\n","        classifier.eval()\n","        for domain in domains:\n","            val_loader = val_loaders[domain]\n","            accuracy, _, _ = evaluate_model(\n","                encoder,\n","                classifier,\n","                decoders[domain],\n","                val_loader,\n","                device,\n","                domain_to_idx[domain],\n","            )\n","            print(f\"Validation Accuracy on {domain}: {accuracy * 100:.2f}%\")\n","\n","        scheduler.step(avg_loss)\n","\n","        if avg_loss < best_loss:\n","            best_loss = avg_loss\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n","                break"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T17:28:21.499958Z","iopub.status.busy":"2024-10-04T17:28:21.499657Z","iopub.status.idle":"2024-10-04T17:28:21.512354Z","shell.execute_reply":"2024-10-04T17:28:21.511482Z","shell.execute_reply.started":"2024-10-04T17:28:21.499927Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(encoder, classifier, decoder, dataloader, device, domain_label):\n","    encoder.eval()\n","    classifier.eval()\n","    decoder.eval()\n","    total_clf_loss = 0.0\n","    total_recon_loss = 0.0\n","    correct = 0\n","    total = 0\n","    clf_loss_fn = nn.CrossEntropyLoss()\n","    \n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            batch_size = inputs.size(0)\n","            mu, logvar = encoder(inputs)\n","            z = reparameterize(mu, logvar)\n","            outputs = classifier(z)\n","            \n","            # Chuyển domain_label thành tensor và lặp lại cho mỗi mẫu trong batch\n","            domain_labels = torch.full((batch_size,), domain_label, device=device)\n","            reconstructed_imgs = decoder(z, domain_labels)\n","\n","            # Classification accuracy\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            # Losses\n","            clf_loss = clf_loss_fn(outputs, labels)\n","            recon_loss = F.mse_loss(reconstructed_imgs, inputs, reduction=\"sum\")\n","            total_clf_loss += clf_loss.item()\n","            total_recon_loss += recon_loss.item()\n","\n","    accuracy = correct / total\n","    avg_clf_loss = total_clf_loss / len(dataloader.dataset)\n","    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n","    return accuracy, avg_clf_loss, avg_recon_loss"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T17:28:21.526195Z","iopub.status.busy":"2024-10-04T17:28:21.525908Z","iopub.status.idle":"2024-10-04T17:49:17.907106Z","shell.execute_reply":"2024-10-04T17:49:17.905635Z","shell.execute_reply.started":"2024-10-04T17:28:21.526165Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Epoch 1/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:07<00:00,  2.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.7842, Recon: 2.4097, Clf: 1.9171, KL: 0.0414\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 2/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:07<00:00,  2.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 1.4642, Recon: 2.0374, Clf: 1.5518, KL: 0.1902\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 3/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:04<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 1.3852, Recon: 1.9024, Clf: 1.4568, KL: 0.2952\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 4/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:03<00:00,  2.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 1.3501, Recon: 1.8440, Clf: 1.4134, KL: 0.3504\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 5/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:04<00:00,  2.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 1.3314, Recon: 1.7929, Clf: 1.3936, KL: 0.3724\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 6/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:04<00:00,  2.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 1.3074, Recon: 1.7632, Clf: 1.3661, KL: 0.3812\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 7/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:03<00:00,  2.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Loss: 1.3169, Recon: 1.7112, Clf: 1.3834, KL: 0.3909\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 8/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:05<00:00,  2.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Loss: 1.2926, Recon: 1.7082, Clf: 1.3536, KL: 0.3894\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 9/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:04<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Loss: 1.2750, Recon: 1.6903, Clf: 1.3344, KL: 0.3847\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 10/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 313/313 [02:04<00:00,  2.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Loss: 1.2685, Recon: 1.6641, Clf: 1.3299, KL: 0.3815\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n","Epoch 11/100\n"]},{"name":"stderr","output_type":"stream","text":["Training:   4%|▍         | 13/313 [00:06<02:26,  2.05it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m combined_dataloader \u001b[38;5;241m=\u001b[39m get_combined_dataloader(DATA_PATH, domains)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train model using progressive domain training\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain_model_progressive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m  \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m  \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdomains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcombined_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m  \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Final evaluation on all domains\u001b[39;00m\n\u001b[1;32m     43\u001b[0m evaluate_on_all_domains(encoder, classifier, decoders, domains, DATA_PATH, device)\n","Cell \u001b[0;32mIn[53], line 53\u001b[0m, in \u001b[0;36mtrain_model_progressive\u001b[0;34m(encoder, decoders, classifier, domains, dataloader, optimizer, scheduler, num_epochs, device, patience)\u001b[0m\n\u001b[1;32m     50\u001b[0m running_clf_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     51\u001b[0m running_kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     54\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m     inputs, labels_a, labels_b, lam \u001b[38;5;241m=\u001b[39m mixup_data(inputs, labels, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n","Cell \u001b[0;32mIn[50], line 38\u001b[0m, in \u001b[0;36mPACSDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 38\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1276\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_brightness(img, brightness_factor)\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m contrast_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1276\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1278\u001b[0m     img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:907\u001b[0m, in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m    905\u001b[0m     _log_api_usage_once(adjust_contrast)\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_contrast(img, contrast_factor)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:83\u001b[0m, in \u001b[0;36madjust_contrast\u001b[0;34m(img, contrast_factor)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m enhancer \u001b[38;5;241m=\u001b[39m ImageEnhance\u001b[38;5;241m.\u001b[39mContrast(img)\n\u001b[0;32m---> 83\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43menhancer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menhance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageEnhance.py:40\u001b[0m, in \u001b[0;36m_Enhance.enhance\u001b[0;34m(self, factor)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21menhance\u001b[39m(\u001b[38;5;28mself\u001b[39m, factor: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Returns an enhanced image.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    :rtype: :py:class:`~PIL.Image.Image`\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegenerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3540\u001b[0m, in \u001b[0;36mblend\u001b[0;34m(im1, im2, alpha)\u001b[0m\n\u001b[1;32m   3538\u001b[0m im1\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   3539\u001b[0m im2\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m-> 3540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im1\u001b[38;5;241m.\u001b[39m_new(\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Main training and evaluation script\n","DATA_PATH = (\n","    \"/kaggle/input/pacs-dataset/kfold\"  # Update this path to your dataset location\n",")\n","latent_dim = 256\n","num_classes = 7  # Update this according to your PACS dataset\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Domains in PACS dataset\n","domains = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n","\n","# Initialize models\n","encoder = Encoder(latent_dim).to(device)\n","decoders = {domain: Decoder(latent_dim, len(domains)).to(device) for domain in domains}\n","classifier = Classifier(latent_dim, num_classes).to(device)\n","\n","# Optimizer and Scheduler\n","params = list(encoder.parameters()) + list(classifier.parameters())\n","for decoder in decoders.values():\n","    params += list(decoder.parameters())\n","optimizer = optim.AdamW(params, lr=1e-4, weight_decay=1e-5)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, mode=\"min\", factor=0.1, patience=5, verbose=True\n",")\n","\n","# Create a mixed DataLoader for all domains\n","mixed_dataloader = get_mixed_dataloader(DATA_PATH, domains)\n","\n","# Create validation loaders for each domain\n","val_loaders = {domain: get_dataloader(DATA_PATH, domain)[1] for domain in domains}\n","\n","# Train model using progressive domain training\n","train_model_progressive(\n","    encoder,\n","    decoders,\n","    classifier,\n","    domains,\n","    mixed_dataloader,\n","    val_loaders,\n","    optimizer,\n","    scheduler,\n","    num_epochs=100,\n","    device=device,\n","    patience=10,\n",")\n","\n","# Final evaluation on the test set for each domain\n","for domain in domains:\n","    print(f\"Evaluating on test set for domain: {domain}\")\n","    evaluate_model(\n","        encoder,\n","        classifier,\n","        decoders[domain],\n","        test_loaders[domain],\n","        device,\n","        domains.index(domain),\n","    )"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T17:49:29.633449Z","iopub.status.busy":"2024-10-04T17:49:29.633043Z","iopub.status.idle":"2024-10-04T17:51:08.852099Z","shell.execute_reply":"2024-10-04T17:51:08.851105Z","shell.execute_reply.started":"2024-10-04T17:49:29.633409Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Final Evaluation on All Domains\n","\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 64/64 [00:22<00:00,  2.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: art_painting\n","  Accuracy: 59.57%\n","  Avg Classification Loss: 0.0377\n","  Avg Reconstruction Loss: 208500.9526\n","\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 74/74 [00:23<00:00,  3.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: cartoon\n","  Accuracy: 71.20%\n","  Avg Classification Loss: 0.0305\n","  Avg Reconstruction Loss: 284906.5195\n","\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 53/53 [00:18<00:00,  2.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: photo\n","  Accuracy: 87.54%\n","  Avg Classification Loss: 0.0194\n","  Avg Reconstruction Loss: 215283.1936\n","\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 123/123 [00:34<00:00,  3.57it/s]"]},{"name":"stdout","output_type":"stream","text":["Domain: sketch\n","  Accuracy: 67.40%\n","  Avg Classification Loss: 0.0307\n","  Avg Reconstruction Loss: 365410.1858\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["evaluate_on_all_domains(encoder, classifier, decoders, domains, DATA_PATH, device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5815134,"sourceId":9545143,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
