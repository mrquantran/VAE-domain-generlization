{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9545143,"sourceType":"datasetVersion","datasetId":5815134}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms, models\nfrom tqdm import tqdm\nfrom PIL import Image\nimport os\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-10-04T11:42:21.346028Z","iopub.execute_input":"2024-10-04T11:42:21.346443Z","iopub.status.idle":"2024-10-04T11:42:21.352046Z","shell.execute_reply.started":"2024-10-04T11:42:21.346402Z","shell.execute_reply":"2024-10-04T11:42:21.351093Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\n\ndef set_random_seeds(seed_value=42):\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n    np.random.seed(seed_value)  # Numpy module.\n    random.seed(seed_value)  # Python random module.\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_random_seeds()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T11:42:21.353740Z","iopub.execute_input":"2024-10-04T11:42:21.354090Z","iopub.status.idle":"2024-10-04T11:42:21.370967Z","shell.execute_reply.started":"2024-10-04T11:42:21.354057Z","shell.execute_reply":"2024-10-04T11:42:21.370134Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"class PACSDataset(Dataset):\n    def __init__(self, root_dir, domain, transform=None):\n        self.root_dir = root_dir\n        self.domain = domain\n        self.transform = transform\n        self.images, self.labels = self._load_images_labels()\n\n    def _load_images_labels(self):\n        image_paths = []\n        labels = []\n        domain_dir = os.path.join(self.root_dir, self.domain)\n        classes = sorted(\n            [\n                d\n                for d in os.listdir(domain_dir)\n                if os.path.isdir(os.path.join(domain_dir, d))\n            ]\n        )\n\n        for label, class_name in enumerate(classes):\n            class_dir = os.path.join(domain_dir, class_name)\n            for image_name in os.listdir(class_dir):\n                if image_name.endswith((\".png\", \".jpg\", \".jpeg\")):\n                    image_paths.append(os.path.join(class_dir, image_name))\n                    labels.append(label)\n\n        return image_paths, labels\n\n    def __len__(self):\n        return len(self.images)  # Return the number of images\n\n    def __getitem__(self, idx):\n        image_path = self.images[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n\ndef get_transform():\n    return transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(\n                brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2\n            ),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n\n\ndef get_combined_dataloader(root_dir, domains, batch_size=32):\n    datasets = [\n        PACSDataset(root_dir, domain, transform=get_transform()) for domain in domains\n    ]\n    combined_dataset = torch.utils.data.ConcatDataset(datasets)\n    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n\n\ndef get_dataloader(root_dir, domain, batch_size=32):\n    dataset = PACSDataset(root_dir, domain, transform=get_transform())\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T11:42:21.372089Z","iopub.execute_input":"2024-10-04T11:42:21.372397Z","iopub.status.idle":"2024-10-04T11:42:21.387309Z","shell.execute_reply.started":"2024-10-04T11:42:21.372364Z","shell.execute_reply":"2024-10-04T11:42:21.386535Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"# Define Encoder, Decoder, Classifier\nclass Encoder(nn.Module):\n    def __init__(self, latent_dim, model_name='efficientnet_b0'):\n        super(Encoder, self).__init__()\n\n        # Load pretrained EfficientNet\n        self.efficientnet = models.efficientnet_b0(pretrained=True)\n\n        # Freeze EfficientNet layers if you do not want to fine-tune\n        for param in self.efficientnet.parameters():\n            param.requires_grad = False\n\n        # Replace the final classifier with linear layers for mu and logvar\n        in_features = self.efficientnet.classifier[1].in_features  # Get the in_features from the classifier\n\n        # Mean (mu) and log-variance (logvar) layers\n        self.fc_mu = nn.Linear(in_features, latent_dim)\n        self.fc_logvar = nn.Linear(in_features, latent_dim)\n\n    def forward(self, x):\n        # Pass input through EfficientNet feature extractor\n        x = self.efficientnet.features(x)  # Use EfficientNet features\n        x = nn.AdaptiveAvgPool2d(1)(x)  # Adapt to a fixed output size (1x1)\n        x = torch.flatten(x, 1)  # Flatten the output from (batch, channels, 1, 1) to (batch, channels)\n\n        # Compute mu and logvar\n        mu = self.fc_mu(x)\n        logvar = self.fc_logvar(x)\n        return mu, logvar\n\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim, num_domains, model_name=\"efficientnet_b0\"):\n        super(Decoder, self).__init__()\n\n        # Pretrained EfficientNet model\n        self.efficientnet = models.efficientnet_b0(pretrained=True)\n\n        # Freeze EfficientNet layers (if you want to fine-tune only the final part)\n        for param in self.efficientnet.parameters():\n            param.requires_grad = False\n\n        # Domain embedding\n        self.domain_embedding = nn.Embedding(num_domains, latent_dim)\n\n        # Replace the first convolutional layer to accept latent_dim channels\n        self.efficientnet.features[0][0] = nn.Conv2d(\n            latent_dim, 32, kernel_size=3, stride=2, padding=1, bias=False\n        )\n\n        # Adjust the classifier to output the correct shape\n        num_ftrs = self.efficientnet.classifier[1].in_features\n        self.efficientnet.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, 512 * 7 * 7), nn.ReLU()\n        )\n\n        # Decoder part (deconvolutional layers to generate images)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n        )\n\n    def forward(self, z, domain_label):\n        # Incorporate domain information\n        domain_embed = self.domain_embedding(domain_label)\n        z = z + domain_embed  # Combine latent vector with domain embedding\n\n        # Reshape z to match EfficientNet input shape\n        z = z.view(-1, z.size(1), 1, 1)\n        z = z.expand(-1, -1, 7, 7)  # Expand to spatial dimensions\n\n        # Pass through EfficientNet\n        x = self.efficientnet.features(z)\n        x = self.efficientnet.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.efficientnet.classifier(x)\n\n        # Reshape and pass through the decoder\n        x = x.view(-1, 512, 7, 7)\n        return self.decoder(x)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, latent_dim, num_classes):\n        super(Classifier, self).__init__()\n        self.fc = nn.Linear(latent_dim, num_classes)\n\n    def forward(self, z):\n        return self.fc(z)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T11:42:21.389223Z","iopub.execute_input":"2024-10-04T11:42:21.389528Z","iopub.status.idle":"2024-10-04T11:42:21.408656Z","shell.execute_reply.started":"2024-10-04T11:42:21.389498Z","shell.execute_reply":"2024-10-04T11:42:21.407952Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"# Reparameterization trick\ndef reparameterize(mu, logvar):\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n\n\n# VAE loss function\ndef vae_loss(recon_x, x, mu, logvar):\n    MSE = F.mse_loss(recon_x, x, reduction=\"sum\")\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return MSE + KLD\n\n\ndef compute_loss(\n    reconstructed_imgs_list,\n    original_imgs,\n    mu,\n    logvar,\n    predicted_labels,\n    true_labels,\n    clf_loss_fn,\n    alpha=1.0,\n    beta=1.0,\n    gamma=1.0,\n):\n    # Reconstruction Loss (MSE loss without normalization)\n    recon_loss = sum(\n        F.mse_loss(recon, original_imgs, reduction=\"sum\")\n        for recon in reconstructed_imgs_list\n    )\n\n    # KL Divergence Loss\n    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    # Classification Loss\n    clf_loss = clf_loss_fn(predicted_labels, true_labels)\n\n    # Total Loss with weights\n    total_loss = alpha * recon_loss + beta * clf_loss + gamma * kld_loss\n    return total_loss, recon_loss.item(), clf_loss.item(), kld_loss.item()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T11:42:21.409668Z","iopub.execute_input":"2024-10-04T11:42:21.409952Z","iopub.status.idle":"2024-10-04T11:42:21.422403Z","shell.execute_reply.started":"2024-10-04T11:42:21.409921Z","shell.execute_reply":"2024-10-04T11:42:21.421642Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"def train_model_progressive(\n    encoder,\n    decoders,\n    classifier,\n    domains,\n    dataloader,\n    optimizer,\n    num_epochs=10,\n    device=\"cuda\",\n):\n    clf_loss_fn = nn.CrossEntropyLoss()\n    domain_to_idx = {domain: idx for idx, domain in enumerate(domains)}\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        encoder.train()\n        classifier.train()\n        for decoder in decoders.values():\n            decoder.train()\n\n        running_loss = 0.0\n        for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            mu, logvar = encoder(inputs)\n            z = reparameterize(mu, logvar)\n\n            reconstructed_imgs_list = []\n            for domain in domains:\n                domain_label = torch.tensor(\n                    [domain_to_idx[domain]] * inputs.size(0), device=device\n                )\n                reconstructed_imgs = decoders[domain](z, domain_label)\n                reconstructed_imgs_list.append(reconstructed_imgs)\n\n            predicted_labels = classifier(z)\n\n            loss, recon_loss, clf_loss, kld_loss = compute_loss(\n                reconstructed_imgs_list,\n                inputs,\n                mu,\n                logvar,\n                predicted_labels,\n                labels,\n                clf_loss_fn,\n            )\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        avg_loss = running_loss / len(dataloader)\n        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-04T11:42:21.494884Z","iopub.execute_input":"2024-10-04T11:42:21.495182Z","iopub.status.idle":"2024-10-04T11:42:21.508319Z","shell.execute_reply.started":"2024-10-04T11:42:21.495150Z","shell.execute_reply":"2024-10-04T11:42:21.507240Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(encoder, classifier, decoder, dataloader, device, domain_label):\n    encoder.eval()\n    classifier.eval()\n    decoder.eval()\n    total_clf_loss = 0.0\n    total_recon_loss = 0.0\n    correct = 0\n    total = 0\n    clf_loss_fn = nn.CrossEntropyLoss()\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            batch_size = inputs.size(0)\n            mu, logvar = encoder(inputs)\n            z = reparameterize(mu, logvar)\n            outputs = classifier(z)\n            \n            # Chuyển domain_label thành tensor và lặp lại cho mỗi mẫu trong batch\n            domain_labels = torch.full((batch_size,), domain_label, device=device)\n            reconstructed_imgs = decoder(z, domain_labels)\n\n            # Classification accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            # Losses\n            clf_loss = clf_loss_fn(outputs, labels)\n            recon_loss = F.mse_loss(reconstructed_imgs, inputs, reduction=\"sum\")\n            total_clf_loss += clf_loss.item()\n            total_recon_loss += recon_loss.item()\n\n    accuracy = correct / total\n    avg_clf_loss = total_clf_loss / len(dataloader.dataset)\n    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n    return accuracy, avg_clf_loss, avg_recon_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-04T11:42:21.527373Z","iopub.execute_input":"2024-10-04T11:42:21.527658Z","iopub.status.idle":"2024-10-04T11:42:21.539062Z","shell.execute_reply.started":"2024-10-04T11:42:21.527627Z","shell.execute_reply":"2024-10-04T11:42:21.538277Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"def evaluate_on_all_domains(encoder, classifier, decoders, domains, data_path, device):\n    print(\"\\nFinal Evaluation on All Domains\\n\")\n    for domain in domains:\n        eval_dataloader = get_dataloader(data_path, domain)\n        domain_label = domains.index(domain)\n        accuracy, avg_clf_loss, avg_recon_loss = evaluate_model(\n            encoder,\n            classifier,\n            decoders[domain],\n            eval_dataloader,\n            device,\n            domain_label,\n        )\n        print(f\"Domain: {domain}\")\n        print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n        print(f\"  Avg Classification Loss: {avg_clf_loss:.4f}\")\n        print(f\"  Avg Reconstruction Loss: {avg_recon_loss:.4f}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-10-04T11:42:21.540912Z","iopub.execute_input":"2024-10-04T11:42:21.541204Z","iopub.status.idle":"2024-10-04T11:42:21.551774Z","shell.execute_reply.started":"2024-10-04T11:42:21.541148Z","shell.execute_reply":"2024-10-04T11:42:21.550851Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"# Main training and evaluation script\nDATA_PATH = (\n    \"/kaggle/input/pacs-dataset/kfold\"  # Update this path to your dataset location\n)\nlatent_dim = 256\nnum_classes = 7  # Update this according to your PACS dataset\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Domains in PACS dataset\ndomains = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n\n# Initialize models outside the loop\nencoder = Encoder(latent_dim).to(device)\ndecoders = {domain: Decoder(latent_dim, len(domains)).to(device) for domain in domains}\nclassifier = Classifier(latent_dim, num_classes).to(device)\n\n# Optimizer\nparams = list(encoder.parameters()) + list(classifier.parameters())\nfor decoder in decoders.values():\n    params += list(decoder.parameters())\noptimizer = optim.Adam(params, lr=1e-4)\n\n# Create a combined DataLoader for all domains\ncombined_dataloader = get_combined_dataloader(DATA_PATH, domains)\n\n# Train model using progressive domain training\ntrain_model_progressive(\n    encoder,\n    decoders,\n    classifier,\n    domains,\n    combined_dataloader,\n    optimizer,\n    num_epochs=10,\n    device=device,\n)\n\n# Final evaluation on all domains\nevaluate_on_all_domains(encoder, classifier, decoders, domains, DATA_PATH, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T11:42:21.552931Z","iopub.execute_input":"2024-10-04T11:42:21.553245Z","iopub.status.idle":"2024-10-04T12:11:30.990763Z","shell.execute_reply.started":"2024-10-04T11:42:21.553213Z","shell.execute_reply":"2024-10-04T12:11:30.989838Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stdout","text":"Using device: cuda\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [02:41<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 46980892.0543\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [02:40<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 38304866.3195\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [02:39<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 35017076.2109\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [02:36<00:00,  1.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 33432980.3866\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [02:45<00:00,  1.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 32169800.0000\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [02:38<00:00,  1.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 31087122.3003\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [03:13<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 30591644.7764\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [02:47<00:00,  1.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 30288445.4393\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [02:42<00:00,  1.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 29747116.5272\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 313/313 [02:44<00:00,  1.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 29391209.7668\n\nFinal Evaluation on All Domains\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:21<00:00,  2.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Domain: art_painting\n  Accuracy: 26.27%\n  Avg Classification Loss: 0.0583\n  Avg Reconstruction Loss: 245778.3926\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 74/74 [00:25<00:00,  2.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Domain: cartoon\n  Accuracy: 28.54%\n  Avg Classification Loss: 0.0556\n  Avg Reconstruction Loss: 276124.8969\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 53/53 [00:17<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Domain: photo\n  Accuracy: 43.47%\n  Avg Classification Loss: 0.0513\n  Avg Reconstruction Loss: 272931.5403\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 123/123 [00:31<00:00,  3.84it/s]","output_type":"stream"},{"name":"stdout","text":"Domain: sketch\n  Accuracy: 37.64%\n  Avg Classification Loss: 0.0489\n  Avg Reconstruction Loss: 168129.9263\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}