{"cells":[{"cell_type":"code","execution_count":66,"metadata":{"_cell_guid":"4f436e96-ef21-4124-b43a-f3a7eb31b6cf","_uuid":"161a57a5-c9a7-47a3-87cd-6d351a6cdb92","collapsed":false,"execution":{"iopub.execute_input":"2024-10-04T19:35:05.116420Z","iopub.status.busy":"2024-10-04T19:35:05.115675Z","iopub.status.idle":"2024-10-04T19:35:05.122194Z","shell.execute_reply":"2024-10-04T19:35:05.121209Z","shell.execute_reply.started":"2024-10-04T19:35:05.116380Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from tqdm import tqdm\n","from PIL import Image\n","import os\n","import torch.nn.functional as F\n","from torch.nn.modules.loss import _WeightedLoss\n","from sklearn.model_selection import train_test_split\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2"]},{"cell_type":"code","execution_count":67,"metadata":{"_cell_guid":"94b1820c-4753-470c-8f95-aab713e66535","_uuid":"013a8b5e-7432-4915-a2c0-9a8f5daaaff1","collapsed":false,"execution":{"iopub.execute_input":"2024-10-04T19:35:05.126220Z","iopub.status.busy":"2024-10-04T19:35:05.125908Z","iopub.status.idle":"2024-10-04T19:35:05.137348Z","shell.execute_reply":"2024-10-04T19:35:05.136531Z","shell.execute_reply.started":"2024-10-04T19:35:05.126188Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import random\n","import numpy as np\n","\n","def set_random_seeds(seed_value=42):\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n","    np.random.seed(seed_value)  # Numpy module.\n","    random.seed(seed_value)  # Python random module.\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","set_random_seeds()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class PACSDataset(Dataset):\n","    def __init__(self, root_dir, domains, transform=None):\n","        self.root_dir = root_dir\n","        self.domains = domains\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        self._load_images_labels()\n","\n","    def _load_images_labels(self):\n","        for domain in self.domains:\n","            domain_dir = os.path.join(self.root_dir, domain)\n","            classes = sorted(\n","                [\n","                    d\n","                    for d in os.listdir(domain_dir)\n","                    if os.path.isdir(os.path.join(domain_dir, d))\n","                ]\n","            )\n","\n","            for label, class_name in enumerate(classes):\n","                class_dir = os.path.join(domain_dir, class_name)\n","                for image_name in os.listdir(class_dir):\n","                    if image_name.endswith((\".png\", \".jpg\", \".jpeg\")):\n","                        self.images.append(os.path.join(class_dir, image_name))\n","                        self.labels.append(label)\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path).convert(\"RGB\")\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","\n","def get_transform():\n","    return transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(10),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0),\n","    ])"]},{"cell_type":"code","execution_count":68,"metadata":{"_cell_guid":"633a724d-b68c-45bb-ab3c-e88e9115dbbc","_uuid":"9cc659f6-3f5a-4ddd-96c2-737a8c49e42a","collapsed":false,"execution":{"iopub.execute_input":"2024-10-04T19:35:05.139160Z","iopub.status.busy":"2024-10-04T19:35:05.138860Z","iopub.status.idle":"2024-10-04T19:35:05.159988Z","shell.execute_reply":"2024-10-04T19:35:05.159115Z","shell.execute_reply.started":"2024-10-04T19:35:05.139129Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def get_dataloader(\n","    root_dir,\n","    domains,\n","    batch_size=32,\n","    train_size=0.7,\n","    val_size=0.15,\n","    test_size=0.15,\n","    random_seed=42,\n","):\n","    # Ensure the sizes sum to 1\n","    assert train_size + val_size + test_size == 1.0\n","\n","    # Load all image paths and their corresponding labels from all domains\n","    image_paths = []\n","    labels = []\n","    for domain in domains:\n","        domain_dir = os.path.join(root_dir, domain)\n","        classes = sorted(\n","            [\n","                d\n","                for d in os.listdir(domain_dir)\n","                if os.path.isdir(os.path.join(domain_dir, d))\n","            ]\n","        )\n","\n","        for label, class_name in enumerate(classes):\n","            class_dir = os.path.join(domain_dir, class_name)\n","            for image_name in os.listdir(class_dir):\n","                if image_name.endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    image_paths.append(os.path.join(class_dir, image_name))\n","                    labels.append(label)\n","\n","    # Split the combined dataset into train, val, and test\n","    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n","        image_paths,\n","        labels,\n","        train_size=train_size,\n","        stratify=labels,\n","        random_state=random_seed,\n","    )\n","    val_paths, test_paths, val_labels, test_labels = train_test_split(\n","        temp_paths,\n","        temp_labels,\n","        train_size=val_size / (val_size + test_size),\n","        stratify=temp_labels,\n","        random_state=random_seed,\n","    )\n","\n","    # Create datasets\n","    train_dataset = PACSDataset(root_dir, domains, transform=get_transform())\n","    train_dataset.images = train_paths\n","    train_dataset.labels = train_labels\n","\n","    val_dataset = PACSDataset(root_dir, domains, transform=get_transform())\n","    val_dataset.images = val_paths\n","    val_dataset.labels = val_labels\n","\n","    test_dataset = PACSDataset(root_dir, domains, transform=get_transform())\n","    test_dataset.images = test_paths\n","    test_dataset.labels = test_labels\n","\n","    # Create DataLoaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":69,"metadata":{"_cell_guid":"77b09e73-481d-48f2-be3d-49a15d8c6777","_uuid":"258e95c8-5d7d-4888-a00d-0a5e99aaa736","collapsed":false,"execution":{"iopub.execute_input":"2024-10-04T19:35:05.161472Z","iopub.status.busy":"2024-10-04T19:35:05.161209Z","iopub.status.idle":"2024-10-04T19:35:05.183260Z","shell.execute_reply":"2024-10-04T19:35:05.182466Z","shell.execute_reply.started":"2024-10-04T19:35:05.161443Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Define Encoder, Decoder, Classifier\n","class Encoder(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(Encoder, self).__init__()\n","\n","        # Sử dụng EfficientNet-B1\n","        self.efficientnet = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.IMAGENET1K_V1)\n","\n","        # Freeze EfficientNet layers\n","        for param in self.efficientnet.parameters():\n","            param.requires_grad = False\n","\n","        # Lấy số features từ lớp cuối cùng của EfficientNet-B1\n","        in_features = self.efficientnet.classifier[1].in_features\n","\n","        # Attention mechanism\n","        self.attention = nn.Sequential(\n","            nn.Linear(in_features, in_features // 16),\n","            nn.ReLU(),\n","            nn.Linear(in_features // 16, in_features),\n","            nn.Sigmoid(),\n","        )\n","\n","        # Mean (mu) and log-variance (logvar) layers\n","        self.fc_mu = nn.Linear(in_features, latent_dim)\n","        self.fc_logvar = nn.Linear(in_features, latent_dim)\n","\n","        self.dropout = nn.Dropout(0.5)  # Add dropout\n","\n","    def forward(self, x):\n","        # Pass input through EfficientNet feature extractor\n","        features = self.efficientnet.features(x)\n","        x = self.efficientnet.avgpool(features)\n","        x = torch.flatten(x, 1)\n","\n","        x = self.dropout(x)  # Apply dropout\n","\n","        # Apply attention\n","        attention_weights = self.attention(x)\n","        x = x * attention_weights\n","\n","        # Compute mu and logvar\n","        mu = self.fc_mu(x)\n","        logvar = self.fc_logvar(x)\n","        return mu, logvar\n","\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.shortcut = nn.Sequential()\n","        if in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","    def forward(self, x):\n","        residual = x\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(residual)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, latent_dim, num_domains):\n","        super(Decoder, self).__init__()\n","\n","        self.domain_embedding = nn.Embedding(num_domains, latent_dim)\n","\n","        self.fc = nn.Linear(latent_dim, 512 * 7 * 7)\n","\n","        self.decoder = nn.Sequential(\n","            ResidualBlock(512, 256),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            ResidualBlock(128, 128),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            ResidualBlock(64, 64),\n","            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 3, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh()  # Thêm Tanh để đảm bảo đầu ra trong khoảng [-1, 1]\n","        )\n","\n","        # Attention mechanism\n","        self.attention = nn.Sequential(nn.Conv2d(3, 1, kernel_size=1), nn.Sigmoid())\n","\n","    def forward(self, z, domain_label):\n","        domain_embed = self.domain_embedding(domain_label)\n","        z = z + domain_embed\n","\n","        x = self.fc(z)\n","        x = x.view(-1, 512, 7, 7)\n","        x = self.decoder(x)\n","\n","        # Apply attention\n","        attention_map = self.attention(x)\n","        x = x * attention_map\n","\n","        return x\n","\n","\n","class Classifier(nn.Module):\n","    def __init__(self, latent_dim, num_classes):\n","        super(Classifier, self).__init__()\n","        self.fc = nn.Linear(latent_dim, num_classes)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, z):\n","        z = self.dropout(z)\n","        return self.fc(z)"]},{"cell_type":"code","execution_count":70,"metadata":{"_cell_guid":"a89bca72-9054-4ba9-8fe9-2b0c6572733e","_uuid":"fe7418a5-f00f-4fb1-8992-1bc42af2c5d3","collapsed":false,"execution":{"iopub.execute_input":"2024-10-04T19:35:05.257653Z","iopub.status.busy":"2024-10-04T19:35:05.256893Z","iopub.status.idle":"2024-10-04T19:35:05.276746Z","shell.execute_reply":"2024-10-04T19:35:05.275794Z","shell.execute_reply.started":"2024-10-04T19:35:05.257614Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class LabelSmoothingLoss(_WeightedLoss):\n","    def __init__(self, weight=None, reduction=\"mean\", smoothing=0.0):\n","        super().__init__(weight=weight, reduction=reduction)\n","        self.smoothing = smoothing\n","        self.weight = weight\n","        self.reduction = reduction\n","\n","    def k_one_hot(self, targets: torch.Tensor, n_classes: int, smoothing=0.0):\n","        with torch.no_grad():\n","            targets = (\n","                torch.empty(size=(targets.size(0), n_classes), device=targets.device)\n","                .fill_(smoothing / (n_classes - 1))\n","                .scatter_(1, targets.data.unsqueeze(1), 1.0 - smoothing)\n","            )\n","        return targets\n","\n","    def reduce_loss(self, loss):\n","        return (\n","            loss.mean()\n","            if self.reduction == \"mean\"\n","            else loss.sum() if self.reduction == \"sum\" else loss\n","        )\n","\n","    def forward(self, inputs, targets):\n","        assert 0 <= self.smoothing < 1\n","\n","        targets = self.k_one_hot(targets, inputs.size(-1), self.smoothing)\n","        log_preds = F.log_softmax(inputs, -1)\n","\n","        if self.weight is not None:\n","            log_preds = log_preds * self.weight.unsqueeze(0)\n","\n","        return self.reduce_loss(-(targets * log_preds).sum(dim=-1))\n","\n","class DynamicWeightBalancer:\n","    def __init__(self, init_alpha=0.5, init_beta=2.0, init_gamma=0.1, patience=5, scaling_factor=0.8):\n","        self.alpha = init_alpha  # Reconstruction loss weight\n","        self.beta = init_beta    # Classification loss weight\n","        self.gamma = init_gamma  # KL divergence weight\n","        self.patience = patience\n","        self.scaling_factor = scaling_factor\n","        self.best_loss = float('inf')\n","        self.counter = 0\n","\n","    def update(self, current_loss, recon_loss, clf_loss, kl_loss):\n","        if current_loss < self.best_loss:\n","            self.best_loss = current_loss\n","            self.counter = 0\n","        else:\n","            self.counter += 1\n","\n","        if self.counter >= self.patience:\n","            self.counter = 0\n","            # Increase classification weight and decrease others\n","            self.beta /= self.scaling_factor\n","            self.alpha *= self.scaling_factor\n","            self.gamma *= self.scaling_factor\n","\n","        # Ensure classification loss weight is always significantly larger\n","        total_weight = self.alpha + self.beta + self.gamma\n","        self.alpha = max(0.1, min(0.3, self.alpha / total_weight))\n","        self.beta = max(0.6, min(0.8, self.beta / total_weight))\n","        self.gamma = 1 - self.alpha - self.beta\n","\n","        return self.alpha, self.beta, self.gamma"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def reparameterize(mu, logvar, dropout_rate=0.5):\n","    std = torch.exp(0.5 * logvar)\n","    eps = torch.randn_like(std)\n","    z = mu + eps * std\n","    z = F.dropout(z, p=dropout_rate, training=True)  # Apply dropout\n","    return z\n","\n","def compute_loss(reconstructed_imgs_list, original_imgs, mu, logvar, predicted_labels, true_labels, clf_loss_fn, epoch, total_epochs, balancer):\n","    recon_loss = sum(\n","        F.mse_loss(recon, original_imgs, reduction=\"mean\")\n","        for recon in reconstructed_imgs_list\n","    ) / len(reconstructed_imgs_list)\n","\n","    kld_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n","    clf_loss = clf_loss_fn(predicted_labels, true_labels)\n","\n","    alpha, beta, gamma = balancer.update(recon_loss + clf_loss + kld_loss, recon_loss, clf_loss, kld_loss)\n","\n","    total_loss = alpha * recon_loss + beta * clf_loss + gamma * kld_loss\n","    return total_loss, recon_loss.item(), clf_loss.item(), kld_loss.item(), alpha, beta, gamma"]},{"cell_type":"code","execution_count":71,"metadata":{"_cell_guid":"5963984d-caaf-4e6c-abff-b2ced9337abb","_uuid":"64e2706d-9120-4121-9358-b00c42985b97","collapsed":false,"execution":{"iopub.execute_input":"2024-10-04T19:35:05.279709Z","iopub.status.busy":"2024-10-04T19:35:05.279094Z","iopub.status.idle":"2024-10-04T19:35:05.300829Z","shell.execute_reply":"2024-10-04T19:35:05.299969Z","shell.execute_reply.started":"2024-10-04T19:35:05.279674Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def mixup_data(x, y, alpha=1.0, device=\"cuda\"):\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).to(device)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","\n","def mixup_criterion(criterion, pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model_progressive(\n","    encoder,\n","    decoders,\n","    classifier,\n","    domains,\n","    dataloader,\n","    val_loaders,\n","    optimizer,\n","    scheduler,\n","    num_epochs=100,\n","    device=\"cuda\",\n","    patience=10,\n","):  \n","    print(\"Training model with progressive domain adaptation\")\n","    print(f\"Number of epochs: {num_epochs}\")\n","    print(f\"Patience: {patience}\")\n","    print(f\"Domains: {domains}\")\n","    print(f\"Device: {device}\")\n","    print(f'Number of training samples: {len(dataloader.dataset)}')\n","    print(f'Number of validation samples: {len(val_loaders[domains[0]].dataset)}')\n","\n","    clf_loss_fn = LabelSmoothingLoss(smoothing=0.1)\n","    domain_to_idx = {domain: idx for idx, domain in enumerate(domains)}\n","\n","    best_loss = float(\"inf\")\n","    patience_counter = 0\n","\n","    balancer = DynamicWeightBalancer()\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","        encoder.train()\n","        classifier.train()\n","        for decoder in decoders.values():\n","            decoder.train()\n","\n","        running_loss = 0.0\n","        running_recon_loss = 0.0\n","        running_clf_loss = 0.0\n","        running_kl_loss = 0.0\n","\n","        for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            inputs, labels_a, labels_b, lam = mixup_data(\n","                inputs, labels, alpha=0.2, device=device\n","            )\n","\n","            mu, logvar = encoder(inputs)\n","            z = reparameterize(mu, logvar)\n","\n","            reconstructed_imgs_list = []\n","            for domain in domains:\n","                domain_label = torch.tensor(\n","                    [domain_to_idx[domain]] * inputs.size(0), device=device\n","                )\n","                reconstructed_imgs = decoders[domain](z, domain_label)\n","                reconstructed_imgs_list.append(reconstructed_imgs)\n","\n","            predicted_labels = classifier(z)\n","\n","            loss, recon_loss, clf_loss, kl_loss, alpha, beta, gamma = compute_loss(\n","                reconstructed_imgs_list,\n","                inputs,\n","                mu,\n","                logvar,\n","                predicted_labels,\n","                labels,\n","                lambda pred, target: mixup_criterion(\n","                    clf_loss_fn, pred, labels_a, labels_b, lam\n","                ),\n","                epoch,\n","                num_epochs,\n","                balancer,\n","            )\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            running_recon_loss += recon_loss\n","            running_clf_loss += clf_loss\n","            running_kl_loss += kl_loss\n","\n","        avg_loss = running_loss / len(dataloader)\n","        avg_recon_loss = running_recon_loss / len(dataloader)\n","        avg_clf_loss = running_clf_loss / len(dataloader)\n","        avg_kl_loss = running_kl_loss / len(dataloader)\n","\n","        print(\n","            f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Recon: {avg_recon_loss:.4f}, Clf: {avg_clf_loss:.4f}, KL: {avg_kl_loss:.4f}\"\n","        )\n","        print(f\"Weights - Alpha: {alpha:.4f}, Beta: {beta:.4f}, Gamma: {gamma:.4f}\")\n","\n","        if (epoch + 1) % 10 == 0:\n","            # Evaluate on all domains\n","            encoder.eval()\n","            classifier.eval()\n","            for domain in domains:\n","                val_loader = val_loaders[domain]\n","                accuracy, _, _ = evaluate_model(\n","                    encoder,\n","                    classifier,\n","                    decoders[domain],\n","                    val_loader,\n","                    device,\n","                    domain_to_idx[domain],\n","                )\n","                print(f\"Validation Accuracy on {domain}: {accuracy * 100:.2f}%\")\n","\n","        scheduler.step(avg_loss)\n","\n","        if avg_loss < best_loss:\n","            best_loss = avg_loss\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n","                break"]},{"cell_type":"code","execution_count":72,"metadata":{"_cell_guid":"2a3716ee-a41f-40f9-be92-6e13201957fc","_uuid":"c9b6247e-a3a8-4139-97cf-47beb2962809","collapsed":false,"execution":{"iopub.execute_input":"2024-10-04T19:35:05.302249Z","iopub.status.busy":"2024-10-04T19:35:05.301954Z","iopub.status.idle":"2024-10-04T19:35:05.323859Z","shell.execute_reply":"2024-10-04T19:35:05.323006Z","shell.execute_reply.started":"2024-10-04T19:35:05.302218Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["def evaluate_model(encoder, classifier, decoder, dataloader, device, domain_label):\n","    encoder.eval()\n","    classifier.eval()\n","    decoder.eval()\n","    total_clf_loss = 0.0\n","    total_recon_loss = 0.0\n","    correct = 0\n","    total = 0\n","    clf_loss_fn = nn.CrossEntropyLoss()\n","    \n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            batch_size = inputs.size(0)\n","            mu, logvar = encoder(inputs)\n","            z = reparameterize(mu, logvar)\n","            outputs = classifier(z)\n","            \n","            # Chuyển domain_label thành tensor và lặp lại cho mỗi mẫu trong batch\n","            domain_labels = torch.full((batch_size,), domain_label, device=device)\n","            reconstructed_imgs = decoder(z, domain_labels)\n","\n","            # Classification accuracy\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            # Losses\n","            clf_loss = clf_loss_fn(outputs, labels)\n","            recon_loss = F.mse_loss(reconstructed_imgs, inputs, reduction=\"sum\")\n","            total_clf_loss += clf_loss.item()\n","            total_recon_loss += recon_loss.item()\n","\n","    accuracy = correct / total\n","    avg_clf_loss = total_clf_loss / len(dataloader.dataset)\n","    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n","    return accuracy, avg_clf_loss, avg_recon_loss"]},{"cell_type":"code","execution_count":73,"metadata":{"_cell_guid":"674a907d-466e-49d6-b8e9-fc6483ee92f3","_uuid":"9339af62-d3b0-4922-b5a6-52ddb95a57f5","collapsed":false,"execution":{"iopub.execute_input":"2024-10-04T19:35:05.325185Z","iopub.status.busy":"2024-10-04T19:35:05.324886Z","iopub.status.idle":"2024-10-04T19:44:50.942348Z","shell.execute_reply":"2024-10-04T19:44:50.940942Z","shell.execute_reply.started":"2024-10-04T19:35:05.325153Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Epoch 1/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 219/219 [01:27<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 1.8766, Recon: 2.4858, Clf: 2.0176, KL: 0.0253\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:21<00:00,  2.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on art_painting: 36.49%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:14<00:00,  3.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on cartoon: 36.56%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on photo: 37.22%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.54it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on sketch: 37.56%\n","Epoch 2/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 219/219 [01:28<00:00,  2.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 1.5592, Recon: 2.1286, Clf: 1.6680, KL: 0.1189\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:15<00:00,  3.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on art_painting: 51.57%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on cartoon: 54.70%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on photo: 53.17%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on sketch: 53.77%\n","Epoch 3/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 219/219 [01:28<00:00,  2.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 1.4371, Recon: 1.9769, Clf: 1.5209, KL: 0.2272\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:14<00:00,  3.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on art_painting: 57.77%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on cartoon: 59.64%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on photo: 59.51%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on sketch: 58.24%\n","Epoch 4/100\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 219/219 [01:28<00:00,  2.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 1.3827, Recon: 1.8973, Clf: 1.4546, KL: 0.2930\n","Weights - Alpha: 0.1000, Beta: 0.8000, Gamma: 0.1000\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:14<00:00,  3.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on art_painting: 60.24%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on cartoon: 61.57%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:14<00:00,  3.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on photo: 61.71%\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 47/47 [00:13<00:00,  3.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy on sketch: 62.04%\n","Epoch 5/100\n"]},{"name":"stderr","output_type":"stream","text":["Training:   3%|▎         | 6/219 [00:02<01:39,  2.15it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[73], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m val_loaders \u001b[38;5;241m=\u001b[39m {domain: val_loader \u001b[38;5;28;01mfor\u001b[39;00m domain \u001b[38;5;129;01min\u001b[39;00m domains}\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train model using the combined DataLoader\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrain_model_progressive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m  \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m  \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdomains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m  \u001b[49m\u001b[43mval_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the dictionary of validation loaders\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m  \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Final evaluation on the test set\u001b[39;00m\n\u001b[1;32m     47\u001b[0m evaluate_model(\n\u001b[1;32m     48\u001b[0m   encoder,\n\u001b[1;32m     49\u001b[0m   classifier,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m   device,\n\u001b[1;32m     53\u001b[0m )\n","Cell \u001b[0;32mIn[71], line 78\u001b[0m, in \u001b[0;36mtrain_model_progressive\u001b[0;34m(encoder, decoders, classifier, domains, dataloader, val_loaders, optimizer, scheduler, num_epochs, device, patience)\u001b[0m\n\u001b[1;32m     76\u001b[0m reconstructed_imgs_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m domain \u001b[38;5;129;01min\u001b[39;00m domains:\n\u001b[0;32m---> 78\u001b[0m     domain_label \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdomain_to_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     reconstructed_imgs \u001b[38;5;241m=\u001b[39m decoders[domain](z, domain_label)\n\u001b[1;32m     82\u001b[0m     reconstructed_imgs_list\u001b[38;5;241m.\u001b[39mappend(reconstructed_imgs)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Main training and evaluation script\n","DATA_PATH = \"/kaggle/input/pacs-dataset/kfold\"  # Update this path to your dataset location\n","latent_dim = 256\n","num_classes = 7  # Update this according to your PACS dataset\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Domains in PACS dataset\n","domains = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n","\n","# Initialize models\n","encoder = Encoder(latent_dim).to(device)\n","decoders = {domain: Decoder(latent_dim, len(domains)).to(device) for domain in domains}\n","classifier = Classifier(latent_dim, num_classes).to(device)\n","\n","# Optimizer and Scheduler\n","params = list(encoder.parameters()) + list(classifier.parameters())\n","for decoder in decoders.values():\n","    params += list(decoder.parameters())\n","optimizer = optim.AdamW(params, lr=1e-4, weight_decay=1e-5)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","   optimizer, mode=\"min\", factor=0.1, patience=5\n",")\n","\n","# Create a single DataLoader for all domains\n","train_loader, val_loader, test_loader = get_dataloader(DATA_PATH, domains)\n","\n","# Create a dictionary for validation loaders if needed\n","val_loaders = {domain: val_loader for domain in domains}\n","\n","# Train model using the combined DataLoader\n","train_model_progressive(\n","  encoder,\n","  decoders,\n","  classifier,\n","  domains,\n","  train_loader,\n","  val_loaders,  # Pass the dictionary of validation loaders\n","  optimizer,\n","  scheduler,\n","  num_epochs=100,\n","  device=device,\n","  patience=10,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f262034e-f894-4254-bf2a-468a053576ba","_uuid":"c9bc6a5c-94e6-4092-89a6-05ce1938018a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5815134,"sourceId":9545143,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
