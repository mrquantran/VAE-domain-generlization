{"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"","display_name":""},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9545143,"sourceType":"datasetVersion","datasetId":5815134}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms, models\nfrom tqdm import tqdm\nfrom PIL import Image\nimport os\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-10-04T08:34:18.930808Z","iopub.execute_input":"2024-10-04T08:34:18.931282Z","iopub.status.idle":"2024-10-04T08:34:18.940277Z","shell.execute_reply.started":"2024-10-04T08:34:18.931232Z","shell.execute_reply":"2024-10-04T08:34:18.939290Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class PACSDataset(Dataset):\n    def __init__(self, root_dir, domain, transform=None):\n        self.root_dir = root_dir\n        self.domain = domain\n        self.transform = transform\n        self.images, self.labels = self._load_images_labels()\n\n    def _load_images_labels(self):\n        image_paths = []\n        labels = []\n        domain_dir = os.path.join(self.root_dir, self.domain)\n        classes = sorted(\n            [\n                d\n                for d in os.listdir(domain_dir)\n                if os.path.isdir(os.path.join(domain_dir, d))\n            ]\n        )\n\n        for label, class_name in enumerate(classes):\n            class_dir = os.path.join(domain_dir, class_name)\n            for image_name in os.listdir(class_dir):\n                if image_name.endswith((\".png\", \".jpg\", \".jpeg\")):\n                    image_paths.append(os.path.join(class_dir, image_name))\n                    labels.append(label)\n\n        return image_paths, labels\n\n    def __len__(self):\n        return len(self.images)  # Return the number of images\n\n    def __getitem__(self, idx):\n        image_path = self.images[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n\n# Function to get DataLoader\ndef get_dataloader(root_dir, domain, batch_size=32):\n    transform = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n    dataset = PACSDataset(root_dir, domain, transform=transform)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T08:34:18.941955Z","iopub.execute_input":"2024-10-04T08:34:18.942795Z","iopub.status.idle":"2024-10-04T08:34:18.955034Z","shell.execute_reply.started":"2024-10-04T08:34:18.942744Z","shell.execute_reply":"2024-10-04T08:34:18.954178Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Define Encoder, Decoder, Classifier\nclass Encoder(nn.Module):\n    def __init__(self, latent_dim):\n        super(Encoder, self).__init__()\n        resnet = models.resnet18(pretrained=True)\n        self.features = nn.Sequential(*list(resnet.children())[:-1])\n        self.fc_mu = nn.Linear(resnet.fc.in_features, latent_dim)\n        self.fc_logvar = nn.Linear(resnet.fc.in_features, latent_dim)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        mu = self.fc_mu(x)\n        logvar = self.fc_logvar(x)\n        return mu, logvar\n\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim):\n        super(Decoder, self).__init__()\n        self.fc = nn.Linear(latent_dim, 512 * 7 * 7)\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, z):\n        z = self.fc(z)\n        z = z.view(-1, 512, 7, 7)\n        return self.decoder(z)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, latent_dim, num_classes):\n        super(Classifier, self).__init__()\n        self.fc = nn.Linear(latent_dim, num_classes)\n\n    def forward(self, z):\n        return self.fc(z)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T08:34:18.956190Z","iopub.execute_input":"2024-10-04T08:34:18.956498Z","iopub.status.idle":"2024-10-04T08:34:18.970016Z","shell.execute_reply.started":"2024-10-04T08:34:18.956465Z","shell.execute_reply":"2024-10-04T08:34:18.969166Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Reparameterization trick\ndef reparameterize(mu, logvar):\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n\n\n# VAE loss function\ndef vae_loss(recon_x, x, mu, logvar):\n    MSE = F.mse_loss(recon_x, x, reduction=\"sum\")\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return MSE + KLD\n\n\ndef compute_loss(\n    reconstructed_imgs_list,\n    original_imgs,\n    mu,\n    logvar,\n    predicted_labels,\n    true_labels,\n    clf_loss_fn,\n):\n    vae_loss_total = 0.0\n    for reconstructed_imgs in reconstructed_imgs_list:\n        vae_loss_val = vae_loss(reconstructed_imgs, original_imgs, mu, logvar)\n        vae_loss_total += vae_loss_val\n    clf_loss = clf_loss_fn(predicted_labels, true_labels)\n    total_loss = vae_loss_total + clf_loss\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-04T08:34:18.972468Z","iopub.execute_input":"2024-10-04T08:34:18.973279Z","iopub.status.idle":"2024-10-04T08:34:18.985930Z","shell.execute_reply.started":"2024-10-04T08:34:18.973244Z","shell.execute_reply":"2024-10-04T08:34:18.985053Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def train_model(\n    encoder,\n    decoders,\n    classifier,\n    source_domain,\n    target_domains,\n    dataloader,\n    optimizer,\n    num_epochs=10,\n    device=\"cuda\",\n):\n    clf_loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        encoder.train()\n        classifier.train()\n        for decoder in decoders.values():\n            decoder.train()\n\n        running_loss = 0.0\n        for inputs, labels in tqdm(dataloader, desc=f\"Training on {source_domain}\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass through encoder to get mu and logvar\n            mu, logvar = encoder(inputs)\n\n            # Reparameterization trick\n            z = reparameterize(mu, logvar)\n\n            # Forward through decoders for each target domain\n            reconstructed_imgs_list = []\n            for domain in target_domains:\n                reconstructed_imgs = decoders[domain](z)\n                reconstructed_imgs_list.append(reconstructed_imgs)\n\n            # Forward through classifier to predict class\n            predicted_labels = classifier(z)\n\n            # Compute loss\n            loss = compute_loss(\n                reconstructed_imgs_list,\n                inputs,\n                mu,\n                logvar,\n                predicted_labels,\n                labels,\n                clf_loss_fn,\n            )\n\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        avg_loss = running_loss / len(dataloader)\n        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-04T08:34:18.986960Z","iopub.execute_input":"2024-10-04T08:34:18.987313Z","iopub.status.idle":"2024-10-04T08:34:18.999712Z","shell.execute_reply.started":"2024-10-04T08:34:18.987264Z","shell.execute_reply":"2024-10-04T08:34:18.998824Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(encoder, classifier, dataloader, device):\n    encoder.eval()\n    classifier.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            mu, logvar = encoder(inputs)\n            z = reparameterize(mu, logvar)\n            outputs = classifier(z)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total","metadata":{"execution":{"iopub.status.busy":"2024-10-04T08:34:19.000820Z","iopub.execute_input":"2024-10-04T08:34:19.001186Z","iopub.status.idle":"2024-10-04T08:34:19.012600Z","shell.execute_reply.started":"2024-10-04T08:34:19.001120Z","shell.execute_reply":"2024-10-04T08:34:19.011692Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Main training and evaluation script\nDATA_PATH = \"/kaggle/input/pacs-dataset/kfold\"  # Update this path to your dataset location\nlatent_dim = 256\nnum_classes = 7  # Update this according to your PACS dataset\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Domains in PACS dataset\ndomains = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n\n# Perform leave-one-out cross-validation\nfor leave_out_domain in domains:\n    print(f\"\\nLeaving out domain: {leave_out_domain}\\n\")\n\n    source_domain = leave_out_domain\n    target_domains = [d for d in domains if d != source_domain]\n\n    # Initialize models\n    encoder = Encoder(latent_dim).to(device)\n    decoders = {domain: Decoder(latent_dim).to(device) for domain in target_domains}\n    classifier = Classifier(latent_dim, num_classes).to(device)\n\n    # Optimizer\n    optimizer = optim.Adam(\n        list(encoder.parameters())\n        + list(classifier.parameters())\n        + [param for decoder in decoders.values() for param in decoder.parameters()],\n        lr=1e-4,\n    )\n\n    # Dataloader for source domain\n    dataloader = get_dataloader(DATA_PATH, source_domain)\n\n    # Train model\n    train_model(\n        encoder,\n        decoders,\n        classifier,\n        source_domain,\n        target_domains,\n        dataloader,\n        optimizer,\n        num_epochs=10,\n        device=device,\n    )\n\n    # Evaluate on target domains\n    for eval_domain in target_domains:\n        eval_dataloader = get_dataloader(DATA_PATH, eval_domain)\n        accuracy = evaluate_model(encoder, classifier, eval_dataloader, device)\n        print(f\"Accuracy on target domain '{eval_domain}': {accuracy * 100:.2f}%\")\n\n# Optionally, you can save the models after training\n# torch.save(encoder.state_dict(), 'encoder.pth')\n# torch.save(classifier.state_dict(), 'classifier.pth')\n# for domain, decoder in decoders.items():\n#     torch.save(decoder.state_dict(), f'decoder_{domain}.pth')","metadata":{"execution":{"iopub.status.busy":"2024-10-04T08:34:19.013956Z","iopub.execute_input":"2024-10-04T08:34:19.014463Z","iopub.status.idle":"2024-10-04T08:48:36.431694Z","shell.execute_reply.started":"2024-10-04T08:34:19.014421Z","shell.execute_reply":"2024-10-04T08:48:36.430687Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Using device: cuda\n\nLeaving out domain: art_painting\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:15<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 21511798.4062\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:14<00:00,  4.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 18961116.8750\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:14<00:00,  4.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 17580366.0156\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:14<00:00,  4.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 16753058.9219\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:14<00:00,  4.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 16165075.5625\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:14<00:00,  4.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 15725671.0156\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:14<00:00,  4.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 15431988.7656\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:14<00:00,  4.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 15218165.7500\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:14<00:00,  4.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 15055520.8906\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:14<00:00,  4.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 14883226.8281\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 74/74 [00:15<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'cartoon': 18.94%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 53/53 [00:07<00:00,  6.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'photo': 23.35%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 123/123 [00:16<00:00,  7.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'sketch': 19.06%\n\nLeaving out domain: cartoon\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:17<00:00,  4.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 35071793.3784\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:17<00:00,  4.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 31609434.5405\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:16<00:00,  4.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 29379053.1149\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:16<00:00,  4.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 27866682.8986\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:16<00:00,  4.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 26862060.9527\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:16<00:00,  4.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 26127331.0811\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:16<00:00,  4.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 25587596.8986\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:16<00:00,  4.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 25190427.0405\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:16<00:00,  4.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 24892841.5608\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:16<00:00,  4.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 24655867.5135\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:08<00:00,  7.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'art_painting': 22.17%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 53/53 [00:06<00:00,  8.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'photo': 23.11%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 123/123 [00:16<00:00,  7.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'sketch': 17.46%\n\nLeaving out domain: photo\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:11<00:00,  4.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 22369355.5094\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:11<00:00,  4.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 20292857.7689\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:11<00:00,  4.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 18679739.3396\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:11<00:00,  4.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 17426323.5236\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:11<00:00,  4.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 16583701.4481\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:12<00:00,  4.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 15948028.8019\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:11<00:00,  4.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 15508020.0991\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:11<00:00,  4.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 15153117.2500\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:11<00:00,  4.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 14873650.5401\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:11<00:00,  4.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 14644519.4858\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:07<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'art_painting': 20.07%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 74/74 [00:09<00:00,  7.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'cartoon': 15.78%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 123/123 [00:16<00:00,  7.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'sketch': 12.42%\n\nLeaving out domain: sketch\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:28<00:00,  4.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 46912727.8699\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:29<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 40609708.6829\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:28<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 36681193.5772\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:29<00:00,  4.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 34556304.5041\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:28<00:00,  4.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 33447207.1707\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:29<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 32818574.1138\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:28<00:00,  4.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 32428526.1789\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:29<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 32169752.6992\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:29<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 31992522.7967\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:29<00:00,  4.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 31863278.5691\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:08<00:00,  7.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'art_painting': 13.72%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 74/74 [00:09<00:00,  8.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'cartoon': 19.11%\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 53/53 [00:06<00:00,  7.94it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'photo': 9.70%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}