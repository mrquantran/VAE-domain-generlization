{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9545143,"sourceType":"datasetVersion","datasetId":5815134}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms, models\nfrom tqdm import tqdm\nfrom PIL import Image\nimport os\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:00:36.219395Z","iopub.execute_input":"2024-10-04T10:00:36.219793Z","iopub.status.idle":"2024-10-04T10:00:36.225780Z","shell.execute_reply.started":"2024-10-04T10:00:36.219756Z","shell.execute_reply":"2024-10-04T10:00:36.224724Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\n\ndef set_random_seeds(seed_value=42):\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n    np.random.seed(seed_value)  # Numpy module.\n    random.seed(seed_value)  # Python random module.\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_random_seeds()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:00:36.227502Z","iopub.execute_input":"2024-10-04T10:00:36.227799Z","iopub.status.idle":"2024-10-04T10:00:36.238971Z","shell.execute_reply.started":"2024-10-04T10:00:36.227767Z","shell.execute_reply":"2024-10-04T10:00:36.238072Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class PACSDataset(Dataset):\n    def __init__(self, root_dir, domain, transform=None):\n        self.root_dir = root_dir\n        self.domain = domain\n        self.transform = transform\n        self.images, self.labels = self._load_images_labels()\n\n    def _load_images_labels(self):\n        image_paths = []\n        labels = []\n        domain_dir = os.path.join(self.root_dir, self.domain)\n        classes = sorted(\n            [\n                d\n                for d in os.listdir(domain_dir)\n                if os.path.isdir(os.path.join(domain_dir, d))\n            ]\n        )\n\n        for label, class_name in enumerate(classes):\n            class_dir = os.path.join(domain_dir, class_name)\n            for image_name in os.listdir(class_dir):\n                if image_name.endswith((\".png\", \".jpg\", \".jpeg\")):\n                    image_paths.append(os.path.join(class_dir, image_name))\n                    labels.append(label)\n\n        return image_paths, labels\n\n    def __len__(self):\n        return len(self.images)  # Return the number of images\n\n    def __getitem__(self, idx):\n        image_path = self.images[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n\n# Function to get DataLoader\ndef get_dataloader(root_dir, domain, batch_size=32):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    dataset = PACSDataset(root_dir, domain, transform=transform)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:00:36.240158Z","iopub.execute_input":"2024-10-04T10:00:36.240524Z","iopub.status.idle":"2024-10-04T10:00:36.253699Z","shell.execute_reply.started":"2024-10-04T10:00:36.240477Z","shell.execute_reply":"2024-10-04T10:00:36.252730Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Define Encoder, Decoder, Classifier\nclass Encoder(nn.Module):\n    def __init__(self, latent_dim):\n        super(Encoder, self).__init__()\n        resnet = models.resnet18(pretrained=True)\n        self.features = nn.Sequential(*list(resnet.children())[:-1])\n        self.fc_mu = nn.Linear(resnet.fc.in_features, latent_dim)\n        self.fc_logvar = nn.Linear(resnet.fc.in_features, latent_dim)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        mu = self.fc_mu(x)\n        logvar = self.fc_logvar(x)\n        return mu, logvar\n\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim, num_domains):\n        super(Decoder, self).__init__()\n        self.fc = nn.Linear(latent_dim, 512 * 7 * 7)\n\n        # Domain embedding\n        self.domain_embedding = nn.Embedding(num_domains, latent_dim)\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, z, domain_label):\n        # Incorporate domain information\n        domain_embed = self.domain_embedding(domain_label)\n        z = z + domain_embed  # Combine latent vector with domain embedding\n        z = self.fc(z)\n        z = z.view(-1, 512, 7, 7)\n        return self.decoder(z)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, latent_dim, num_classes):\n        super(Classifier, self).__init__()\n        self.fc = nn.Linear(latent_dim, num_classes)\n\n    def forward(self, z):\n        return self.fc(z)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:00:36.255071Z","iopub.execute_input":"2024-10-04T10:00:36.255423Z","iopub.status.idle":"2024-10-04T10:00:36.271091Z","shell.execute_reply.started":"2024-10-04T10:00:36.255389Z","shell.execute_reply":"2024-10-04T10:00:36.270173Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Reparameterization trick\ndef reparameterize(mu, logvar):\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n\n\n# VAE loss function\ndef vae_loss(recon_x, x, mu, logvar):\n    MSE = F.mse_loss(recon_x, x, reduction=\"sum\")\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return MSE + KLD\n\n\ndef compute_loss(\n    reconstructed_imgs_list,\n    original_imgs,\n    mu,\n    logvar,\n    predicted_labels,\n    true_labels,\n    clf_loss_fn,\n    alpha=1.0,\n    beta=1.0,\n    gamma=1.0,\n):\n    # Reconstruction Loss\n    recon_loss = sum(\n        F.mse_loss(recon, original_imgs, reduction=\"sum\")\n        for recon in reconstructed_imgs_list\n    )\n    \n    # KL Divergence Loss\n    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    \n    # Classification Loss\n    clf_loss = clf_loss_fn(predicted_labels, true_labels)\n    \n    # Total Loss with weights\n    total_loss = alpha * recon_loss + beta * clf_loss + gamma * kld_loss\n    return total_loss, recon_loss.item(), clf_loss.item(), kld_loss.item()","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:00:36.274089Z","iopub.execute_input":"2024-10-04T10:00:36.274509Z","iopub.status.idle":"2024-10-04T10:00:36.286430Z","shell.execute_reply.started":"2024-10-04T10:00:36.274471Z","shell.execute_reply":"2024-10-04T10:00:36.285554Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train_model(\n    encoder,\n    decoders,\n    classifier,\n    source_domain,\n    target_domains,\n    dataloader,\n    optimizer,\n    num_epochs=10,\n    device=\"cuda\",\n):\n    clf_loss_fn = nn.CrossEntropyLoss()\n    domain_to_idx = {domain: idx for idx, domain in enumerate(target_domains)}\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        encoder.train()\n        classifier.train()\n        for decoder in decoders.values():\n            decoder.train()\n\n        running_loss = 0.0\n        for inputs, labels in tqdm(dataloader, desc=f\"Training on {source_domain}\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            mu, logvar = encoder(inputs)\n            z = reparameterize(mu, logvar)\n\n            reconstructed_imgs_list = []\n            for domain in target_domains:\n                domain_label = torch.tensor([domain_to_idx[domain]] * inputs.size(0), device=device)\n                reconstructed_imgs = decoders[domain](z, domain_label)\n                reconstructed_imgs_list.append(reconstructed_imgs)\n\n            predicted_labels = classifier(z)\n\n            loss, recon_loss, clf_loss, kld_loss = compute_loss(\n                reconstructed_imgs_list,\n                inputs,\n                mu,\n                logvar,\n                predicted_labels,\n                labels,\n                clf_loss_fn,\n            )\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        avg_loss = running_loss / len(dataloader)\n        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:00:36.287605Z","iopub.execute_input":"2024-10-04T10:00:36.287903Z","iopub.status.idle":"2024-10-04T10:00:36.298678Z","shell.execute_reply.started":"2024-10-04T10:00:36.287871Z","shell.execute_reply":"2024-10-04T10:00:36.297794Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(encoder, classifier, decoder, dataloader, device, domain_label):\n    encoder.eval()\n    classifier.eval()\n    decoder.eval()\n    total_clf_loss = 0.0\n    total_recon_loss = 0.0\n    correct = 0\n    total = 0\n    clf_loss_fn = nn.CrossEntropyLoss()\n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            batch_size = inputs.size(0)\n            mu, logvar = encoder(inputs)\n            z = reparameterize(mu, logvar)\n            outputs = classifier(z)\n            # Chuyển domain_label thành tensor và lặp lại cho mỗi mẫu trong batch\n            domain_labels = torch.full((batch_size,), domain_label, device=device)\n            reconstructed_imgs = decoder(z, domain_labels)\n            \n            # Classification accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            # Losses\n            clf_loss = clf_loss_fn(outputs, labels)\n            recon_loss = F.mse_loss(reconstructed_imgs, inputs, reduction=\"sum\")\n            total_clf_loss += clf_loss.item()\n            total_recon_loss += recon_loss.item()\n\n    accuracy = correct / total\n    avg_clf_loss = total_clf_loss / len(dataloader.dataset)\n    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n    return accuracy, avg_clf_loss, avg_recon_loss","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:00:36.299795Z","iopub.execute_input":"2024-10-04T10:00:36.300209Z","iopub.status.idle":"2024-10-04T10:00:36.312378Z","shell.execute_reply.started":"2024-10-04T10:00:36.300170Z","shell.execute_reply":"2024-10-04T10:00:36.311393Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def evaluate_on_all_domains(encoder, classifier, decoders, domains, data_path, device):\n    print(\"\\nFinal Evaluation on All Domains\\n\")\n    for domain in domains:\n        print(f'\\nEvaluting on domain {domain}')\n        eval_dataloader = get_dataloader(data_path, domain)\n        domain_label = domains.index(domain)\n        accuracy, avg_clf_loss, avg_recon_loss = evaluate_model(\n            encoder,\n            classifier,\n            decoders[domain],\n            eval_dataloader,\n            device,\n            domain_label,\n        )\n        print(f\"Domain: {domain}\")\n        print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n        print(f\"  Avg Classification Loss: {avg_clf_loss:.4f}\")\n        print(f\"  Avg Reconstruction Loss: {avg_recon_loss:.4f}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:28:41.042875Z","iopub.execute_input":"2024-10-04T10:28:41.043680Z","iopub.status.idle":"2024-10-04T10:28:41.050645Z","shell.execute_reply.started":"2024-10-04T10:28:41.043634Z","shell.execute_reply":"2024-10-04T10:28:41.049406Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Main training and evaluation script\nDATA_PATH = \"/kaggle/input/pacs-dataset/kfold\"  # Update this path to your dataset location\nlatent_dim = 256\nnum_classes = 7  # Update this according to your PACS dataset\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Domains in PACS dataset\ndomains = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n\n# Initialize models outside the loop\nencoder = Encoder(latent_dim).to(device)\ndecoders = {domain: Decoder(latent_dim, len(domains)).to(device) for domain in domains}\nclassifier = Classifier(latent_dim, num_classes).to(device)\n\n# Optimizer\nparams = list(encoder.parameters()) + list(classifier.parameters())\nfor decoder in decoders.values():\n    params += list(decoder.parameters())\noptimizer = optim.Adam(params, lr=1e-4)\n\nfor source_domain in domains:\n    print(f\"\\nTraining on source domain: {source_domain}\\n\")\n\n    target_domains = [d for d in domains if d != source_domain]\n\n    # Dataloader for source domain\n    dataloader = get_dataloader(DATA_PATH, source_domain)\n\n    # Train model\n    train_model(\n        encoder,\n        decoders,\n        classifier,\n        source_domain,\n        target_domains,\n        dataloader,\n        optimizer,\n        num_epochs=10,\n        device=device,\n    )\n\n    # Evaluate model\n    for eval_domain in target_domains:\n        eval_dataloader = get_dataloader(DATA_PATH, eval_domain)\n        domain_label = domains.index(eval_domain)\n        accuracy, avg_clf_loss, avg_recon_loss = evaluate_model(\n            encoder,\n            classifier,\n            decoders[eval_domain],\n            eval_dataloader,\n            device,\n            domain_label,\n        )\n        print(f\"Accuracy on target domain '{eval_domain}': {accuracy * 100:.2f}%\")\n        print(f\"Avg Classification Loss: {avg_clf_loss:.4f}\")\n        print(f\"Avg Reconstruction Loss: {avg_recon_loss:.4f}\")\n\n# Final evaluation on all domains\nevaluate_on_all_domains(encoder, classifier, decoders, domains, DATA_PATH, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:00:36.328105Z","iopub.execute_input":"2024-10-04T10:00:36.328439Z","iopub.status.idle":"2024-10-04T10:27:28.668867Z","shell.execute_reply.started":"2024-10-04T10:00:36.328405Z","shell.execute_reply":"2024-10-04T10:27:28.667786Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Using device: cuda\n\nTraining on source domain: art_painting\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 23728388.8438\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 21367374.5938\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 19837006.9844\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 19033064.9844\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:27<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 18444755.2969\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 17795944.3906\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:27<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 17513212.9375\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 17236755.0625\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:27<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 17038124.6250\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training on art_painting: 100%|██████████| 64/64 [00:27<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 16966236.1250\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 74/74 [00:23<00:00,  3.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'cartoon': 20.09%\nAvg Classification Loss: 0.0611\nAvg Reconstruction Loss: 250982.6343\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 53/53 [00:19<00:00,  2.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'photo': 31.62%\nAvg Classification Loss: 0.0585\nAvg Reconstruction Loss: 184420.6982\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 123/123 [00:35<00:00,  3.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'sketch': 19.22%\nAvg Classification Loss: 0.0608\nAvg Reconstruction Loss: 274486.5206\n\nTraining on source domain: cartoon\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:30<00:00,  2.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 27017963.3243\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 24826419.6351\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 23949692.7500\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 23280479.7500\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 22916207.2635\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 22655107.7162\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 22527150.2905\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 21974861.1351\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 22252388.3716\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 21927625.9054\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:21<00:00,  2.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'art_painting': 27.64%\nAvg Classification Loss: 0.0607\nAvg Reconstruction Loss: 169889.6614\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 53/53 [00:17<00:00,  3.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'photo': 30.84%\nAvg Classification Loss: 0.0638\nAvg Reconstruction Loss: 201114.0524\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 123/123 [00:29<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'sketch': 21.33%\nAvg Classification Loss: 0.0594\nAvg Reconstruction Loss: 281876.7296\n\nTraining on source domain: photo\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 17302811.0425\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 16289796.3443\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:22<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 16085445.5142\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 16026506.6887\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 15820948.0660\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:24<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 15869980.8538\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 15571356.0142\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:22<00:00,  2.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 15664235.4009\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 15735767.0660\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training on photo: 100%|██████████| 53/53 [00:22<00:00,  2.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 15876578.2830\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:21<00:00,  3.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'art_painting': 21.04%\nAvg Classification Loss: 0.0619\nAvg Reconstruction Loss: 167008.2710\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 74/74 [00:21<00:00,  3.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'cartoon': 25.17%\nAvg Classification Loss: 0.0608\nAvg Reconstruction Loss: 230779.5275\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 123/123 [00:29<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'sketch': 14.35%\nAvg Classification Loss: 0.0671\nAvg Reconstruction Loss: 268746.9290\n\nTraining on source domain: sketch\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 28974552.4878\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 26806441.5772\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 26073576.7805\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:42<00:00,  2.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 26117777.0894\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 25471309.1545\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Loss: 25485403.3171\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:42<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Loss: 25741038.9919\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Loss: 25534879.5285\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9, Loss: 25268298.9919\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10, Loss: 25224703.5610\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:21<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'art_painting': 10.40%\nAvg Classification Loss: 0.2640\nAvg Reconstruction Loss: 230264.0049\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 74/74 [00:21<00:00,  3.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'cartoon': 20.52%\nAvg Classification Loss: 0.1155\nAvg Reconstruction Loss: 256191.2451\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 53/53 [00:17<00:00,  3.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy on target domain 'photo': 12.46%\nAvg Classification Loss: 0.2692\nAvg Reconstruction Loss: 302364.4510\n\nFinal Evaluation on All Domains\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:21<00:00,  3.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Domain: art_painting\n  Accuracy: 10.30%\n  Avg Classification Loss: 0.2660\n  Avg Reconstruction Loss: 231631.0410\n\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 123/123 [00:29<00:00,  4.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Domain: sketch\n  Accuracy: 42.10%\n  Avg Classification Loss: 0.0456\n  Avg Reconstruction Loss: 360168.9527\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Final evaluation on all domains\nevaluate_on_all_domains(encoder, classifier, decoders, domains, DATA_PATH, device)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T10:28:47.900087Z","iopub.execute_input":"2024-10-04T10:28:47.900520Z","iopub.status.idle":"2024-10-04T10:30:18.525745Z","shell.execute_reply.started":"2024-10-04T10:28:47.900479Z","shell.execute_reply":"2024-10-04T10:30:18.524701Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\nFinal Evaluation on All Domains\n\n\nEvaluting on domain art_painting\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 64/64 [00:22<00:00,  2.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Domain: art_painting\n  Accuracy: 10.55%\n  Avg Classification Loss: 0.2622\n  Avg Reconstruction Loss: 230154.9268\n\n\nEvaluting on domain cartoon\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 74/74 [00:21<00:00,  3.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Domain: cartoon\n  Accuracy: 20.86%\n  Avg Classification Loss: 0.1150\n  Avg Reconstruction Loss: 252531.8223\n\n\nEvaluting on domain photo\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 53/53 [00:17<00:00,  3.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Domain: photo\n  Accuracy: 12.34%\n  Avg Classification Loss: 0.2696\n  Avg Reconstruction Loss: 300694.8666\n\n\nEvaluting on domain sketch\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 123/123 [00:30<00:00,  4.07it/s]","output_type":"stream"},{"name":"stdout","text":"Domain: sketch\n  Accuracy: 42.86%\n  Avg Classification Loss: 0.0457\n  Avg Reconstruction Loss: 362928.5531\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}