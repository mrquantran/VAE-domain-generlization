{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:00:36.219793Z","iopub.status.busy":"2024-10-04T10:00:36.219395Z","iopub.status.idle":"2024-10-04T10:00:36.225780Z","shell.execute_reply":"2024-10-04T10:00:36.224724Z","shell.execute_reply.started":"2024-10-04T10:00:36.219756Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms, models\n","from tqdm import tqdm\n","from PIL import Image\n","import os\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:00:36.227799Z","iopub.status.busy":"2024-10-04T10:00:36.227502Z","iopub.status.idle":"2024-10-04T10:00:36.238971Z","shell.execute_reply":"2024-10-04T10:00:36.238072Z","shell.execute_reply.started":"2024-10-04T10:00:36.227767Z"},"trusted":true},"outputs":[],"source":["import random\n","import numpy as np\n","\n","def set_random_seeds(seed_value=42):\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)  # if you are using multi-GPU.\n","    np.random.seed(seed_value)  # Numpy module.\n","    random.seed(seed_value)  # Python random module.\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","set_random_seeds()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:00:36.240524Z","iopub.status.busy":"2024-10-04T10:00:36.240158Z","iopub.status.idle":"2024-10-04T10:00:36.253699Z","shell.execute_reply":"2024-10-04T10:00:36.252730Z","shell.execute_reply.started":"2024-10-04T10:00:36.240477Z"},"trusted":true},"outputs":[],"source":["class PACSDataset(Dataset):\n","    def __init__(self, root_dir, domain, transform=None):\n","        self.root_dir = root_dir\n","        self.domain = domain\n","        self.transform = transform\n","        self.images, self.labels = self._load_images_labels()\n","\n","    def _load_images_labels(self):\n","        image_paths = []\n","        labels = []\n","        domain_dir = os.path.join(self.root_dir, self.domain)\n","        classes = sorted(\n","            [\n","                d\n","                for d in os.listdir(domain_dir)\n","                if os.path.isdir(os.path.join(domain_dir, d))\n","            ]\n","        )\n","\n","        for label, class_name in enumerate(classes):\n","            class_dir = os.path.join(domain_dir, class_name)\n","            for image_name in os.listdir(class_dir):\n","                if image_name.endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    image_paths.append(os.path.join(class_dir, image_name))\n","                    labels.append(label)\n","\n","        return image_paths, labels\n","\n","    def __len__(self):\n","        return len(self.images)  # Return the number of images\n","\n","    def __getitem__(self, idx):\n","        image_path = self.images[idx]\n","        image = Image.open(image_path).convert(\"RGB\")\n","        label = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","\n","# Function to get DataLoader\n","def get_dataloader(root_dir, domain, batch_size=32):\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(10),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","    \n","    dataset = PACSDataset(root_dir, domain, transform=transform)\n","    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:00:36.255423Z","iopub.status.busy":"2024-10-04T10:00:36.255071Z","iopub.status.idle":"2024-10-04T10:00:36.271091Z","shell.execute_reply":"2024-10-04T10:00:36.270173Z","shell.execute_reply.started":"2024-10-04T10:00:36.255389Z"},"trusted":true},"outputs":[],"source":["# Define Encoder, Decoder, Classifier\n","class Encoder(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(Encoder, self).__init__()\n","        resnet = models.resnet18(pretrained=True)\n","        self.features = nn.Sequential(*list(resnet.children())[:-1])\n","        self.fc_mu = nn.Linear(resnet.fc.in_features, latent_dim)\n","        self.fc_logvar = nn.Linear(resnet.fc.in_features, latent_dim)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        mu = self.fc_mu(x)\n","        logvar = self.fc_logvar(x)\n","        return mu, logvar\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, latent_dim, num_domains):\n","        super(Decoder, self).__init__()\n","        self.fc = nn.Linear(latent_dim, 512 * 7 * 7)\n","\n","        # Domain embedding\n","        self.domain_embedding = nn.Embedding(num_domains, latent_dim)\n","\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, z, domain_label):\n","        # Incorporate domain information\n","        domain_embed = self.domain_embedding(domain_label)\n","        z = z + domain_embed  # Combine latent vector with domain embedding\n","        z = self.fc(z)\n","        z = z.view(-1, 512, 7, 7)\n","        return self.decoder(z)\n","\n","\n","class Classifier(nn.Module):\n","    def __init__(self, latent_dim, num_classes):\n","        super(Classifier, self).__init__()\n","        self.fc = nn.Linear(latent_dim, num_classes)\n","\n","    def forward(self, z):\n","        return self.fc(z)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:00:36.274509Z","iopub.status.busy":"2024-10-04T10:00:36.274089Z","iopub.status.idle":"2024-10-04T10:00:36.286430Z","shell.execute_reply":"2024-10-04T10:00:36.285554Z","shell.execute_reply.started":"2024-10-04T10:00:36.274471Z"},"trusted":true},"outputs":[],"source":["# Reparameterization trick\n","def reparameterize(mu, logvar):\n","    std = torch.exp(0.5 * logvar)\n","    eps = torch.randn_like(std)\n","    return mu + eps * std\n","\n","\n","# VAE loss function\n","def vae_loss(recon_x, x, mu, logvar):\n","    MSE = F.mse_loss(recon_x, x, reduction=\"sum\")\n","    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    return MSE + KLD\n","\n","\n","def compute_loss(\n","    reconstructed_imgs_list,\n","    original_imgs,\n","    mu,\n","    logvar,\n","    predicted_labels,\n","    true_labels,\n","    clf_loss_fn,\n","    alpha=1.0,\n","    beta=1.0,\n","    gamma=1.0,\n","):\n","    # Reconstruction Loss\n","    recon_loss = sum(\n","        F.mse_loss(recon, original_imgs, reduction=\"sum\")\n","        for recon in reconstructed_imgs_list\n","    )\n","    \n","    # KL Divergence Loss\n","    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n","    \n","    # Classification Loss\n","    clf_loss = clf_loss_fn(predicted_labels, true_labels)\n","    \n","    # Total Loss with weights\n","    total_loss = alpha * recon_loss + beta * clf_loss + gamma * kld_loss\n","    return total_loss, recon_loss.item(), clf_loss.item(), kld_loss.item()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:00:36.287903Z","iopub.status.busy":"2024-10-04T10:00:36.287605Z","iopub.status.idle":"2024-10-04T10:00:36.298678Z","shell.execute_reply":"2024-10-04T10:00:36.297794Z","shell.execute_reply.started":"2024-10-04T10:00:36.287871Z"},"trusted":true},"outputs":[],"source":["def train_model(\n","    encoder,\n","    decoders,\n","    classifier,\n","    source_domain,\n","    target_domains,\n","    dataloader,\n","    optimizer,\n","    num_epochs=10,\n","    device=\"cuda\",\n","):\n","    clf_loss_fn = nn.CrossEntropyLoss()\n","    domain_to_idx = {domain: idx for idx, domain in enumerate(target_domains)}\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        encoder.train()\n","        classifier.train()\n","        for decoder in decoders.values():\n","            decoder.train()\n","\n","        running_loss = 0.0\n","        for inputs, labels in tqdm(dataloader, desc=f\"Training on {source_domain}\"):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            mu, logvar = encoder(inputs)\n","            z = reparameterize(mu, logvar)\n","\n","            reconstructed_imgs_list = []\n","            for domain in target_domains:\n","                domain_label = torch.tensor([domain_to_idx[domain]] * inputs.size(0), device=device)\n","                reconstructed_imgs = decoders[domain](z, domain_label)\n","                reconstructed_imgs_list.append(reconstructed_imgs)\n","\n","            predicted_labels = classifier(z)\n","\n","            loss, recon_loss, clf_loss, kld_loss = compute_loss(\n","                reconstructed_imgs_list,\n","                inputs,\n","                mu,\n","                logvar,\n","                predicted_labels,\n","                labels,\n","                clf_loss_fn,\n","            )\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        avg_loss = running_loss / len(dataloader)\n","        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:00:36.300209Z","iopub.status.busy":"2024-10-04T10:00:36.299795Z","iopub.status.idle":"2024-10-04T10:00:36.312378Z","shell.execute_reply":"2024-10-04T10:00:36.311393Z","shell.execute_reply.started":"2024-10-04T10:00:36.300170Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(encoder, classifier, decoder, dataloader, device, domain_label):\n","    encoder.eval()\n","    classifier.eval()\n","    decoder.eval()\n","    total_clf_loss = 0.0\n","    total_recon_loss = 0.0\n","    correct = 0\n","    total = 0\n","    clf_loss_fn = nn.CrossEntropyLoss()\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            batch_size = inputs.size(0)\n","            mu, logvar = encoder(inputs)\n","            z = reparameterize(mu, logvar)\n","            outputs = classifier(z)\n","            # Chuyển domain_label thành tensor và lặp lại cho mỗi mẫu trong batch\n","            domain_labels = torch.full((batch_size,), domain_label, device=device)\n","            reconstructed_imgs = decoder(z, domain_labels)\n","            \n","            # Classification accuracy\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","            \n","            # Losses\n","            clf_loss = clf_loss_fn(outputs, labels)\n","            recon_loss = F.mse_loss(reconstructed_imgs, inputs, reduction=\"sum\")\n","            total_clf_loss += clf_loss.item()\n","            total_recon_loss += recon_loss.item()\n","\n","    accuracy = correct / total\n","    avg_clf_loss = total_clf_loss / len(dataloader.dataset)\n","    avg_recon_loss = total_recon_loss / len(dataloader.dataset)\n","    return accuracy, avg_clf_loss, avg_recon_loss"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:28:41.043680Z","iopub.status.busy":"2024-10-04T10:28:41.042875Z","iopub.status.idle":"2024-10-04T10:28:41.050645Z","shell.execute_reply":"2024-10-04T10:28:41.049406Z","shell.execute_reply.started":"2024-10-04T10:28:41.043634Z"},"trusted":true},"outputs":[],"source":["def evaluate_on_all_domains(encoder, classifier, decoders, domains, data_path, device):\n","    print(\"\\nFinal Evaluation on All Domains\\n\")\n","    for domain in domains:\n","        print(f'\\nEvaluting on domain {domain}')\n","        eval_dataloader = get_dataloader(data_path, domain)\n","        domain_label = domains.index(domain)\n","        accuracy, avg_clf_loss, avg_recon_loss = evaluate_model(\n","            encoder,\n","            classifier,\n","            decoders[domain],\n","            eval_dataloader,\n","            device,\n","            domain_label,\n","        )\n","        print(f\"Domain: {domain}\")\n","        print(f\"  Accuracy: {accuracy * 100:.2f}%\")\n","        print(f\"  Avg Classification Loss: {avg_clf_loss:.4f}\")\n","        print(f\"  Avg Reconstruction Loss: {avg_recon_loss:.4f}\\n\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:00:36.328439Z","iopub.status.busy":"2024-10-04T10:00:36.328105Z","iopub.status.idle":"2024-10-04T10:27:28.668867Z","shell.execute_reply":"2024-10-04T10:27:28.667786Z","shell.execute_reply.started":"2024-10-04T10:00:36.328405Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","\n","Training on source domain: art_painting\n","\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 23728388.8438\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 21367374.5938\n","Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 19837006.9844\n","Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 19033064.9844\n","Epoch 5/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:27<00:00,  2.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 18444755.2969\n","Epoch 6/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 17795944.3906\n","Epoch 7/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:27<00:00,  2.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Loss: 17513212.9375\n","Epoch 8/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:28<00:00,  2.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Loss: 17236755.0625\n","Epoch 9/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:27<00:00,  2.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Loss: 17038124.6250\n","Epoch 10/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on art_painting: 100%|██████████| 64/64 [00:27<00:00,  2.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Loss: 16966236.1250\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 74/74 [00:23<00:00,  3.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'cartoon': 20.09%\n","Avg Classification Loss: 0.0611\n","Avg Reconstruction Loss: 250982.6343\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 53/53 [00:19<00:00,  2.68it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'photo': 31.62%\n","Avg Classification Loss: 0.0585\n","Avg Reconstruction Loss: 184420.6982\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 123/123 [00:35<00:00,  3.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'sketch': 19.22%\n","Avg Classification Loss: 0.0608\n","Avg Reconstruction Loss: 274486.5206\n","\n","Training on source domain: cartoon\n","\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:30<00:00,  2.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 27017963.3243\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 24826419.6351\n","Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 23949692.7500\n","Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 23280479.7500\n","Epoch 5/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 22916207.2635\n","Epoch 6/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 22655107.7162\n","Epoch 7/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Loss: 22527150.2905\n","Epoch 8/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Loss: 21974861.1351\n","Epoch 9/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Loss: 22252388.3716\n","Epoch 10/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on cartoon: 100%|██████████| 74/74 [00:29<00:00,  2.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Loss: 21927625.9054\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 64/64 [00:21<00:00,  2.94it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'art_painting': 27.64%\n","Avg Classification Loss: 0.0607\n","Avg Reconstruction Loss: 169889.6614\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 53/53 [00:17<00:00,  3.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'photo': 30.84%\n","Avg Classification Loss: 0.0638\n","Avg Reconstruction Loss: 201114.0524\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 123/123 [00:29<00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'sketch': 21.33%\n","Avg Classification Loss: 0.0594\n","Avg Reconstruction Loss: 281876.7296\n","\n","Training on source domain: photo\n","\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 17302811.0425\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 16289796.3443\n","Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:22<00:00,  2.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 16085445.5142\n","Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 16026506.6887\n","Epoch 5/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 15820948.0660\n","Epoch 6/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:24<00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 15869980.8538\n","Epoch 7/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Loss: 15571356.0142\n","Epoch 8/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:22<00:00,  2.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Loss: 15664235.4009\n","Epoch 9/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:23<00:00,  2.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Loss: 15735767.0660\n","Epoch 10/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on photo: 100%|██████████| 53/53 [00:22<00:00,  2.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Loss: 15876578.2830\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 64/64 [00:21<00:00,  3.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'art_painting': 21.04%\n","Avg Classification Loss: 0.0619\n","Avg Reconstruction Loss: 167008.2710\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 74/74 [00:21<00:00,  3.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'cartoon': 25.17%\n","Avg Classification Loss: 0.0608\n","Avg Reconstruction Loss: 230779.5275\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 123/123 [00:29<00:00,  4.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'sketch': 14.35%\n","Avg Classification Loss: 0.0671\n","Avg Reconstruction Loss: 268746.9290\n","\n","Training on source domain: sketch\n","\n","Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 28974552.4878\n","Epoch 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 26806441.5772\n","Epoch 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 26073576.7805\n","Epoch 4/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:42<00:00,  2.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 26117777.0894\n","Epoch 5/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 25471309.1545\n","Epoch 6/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 25485403.3171\n","Epoch 7/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:42<00:00,  2.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Loss: 25741038.9919\n","Epoch 8/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Loss: 25534879.5285\n","Epoch 9/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Loss: 25268298.9919\n","Epoch 10/10\n"]},{"name":"stderr","output_type":"stream","text":["Training on sketch: 100%|██████████| 123/123 [00:43<00:00,  2.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Loss: 25224703.5610\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 64/64 [00:21<00:00,  3.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'art_painting': 10.40%\n","Avg Classification Loss: 0.2640\n","Avg Reconstruction Loss: 230264.0049\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 74/74 [00:21<00:00,  3.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'cartoon': 20.52%\n","Avg Classification Loss: 0.1155\n","Avg Reconstruction Loss: 256191.2451\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 53/53 [00:17<00:00,  3.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy on target domain 'photo': 12.46%\n","Avg Classification Loss: 0.2692\n","Avg Reconstruction Loss: 302364.4510\n","\n","Final Evaluation on All Domains\n","\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 64/64 [00:21<00:00,  3.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: art_painting\n","  Accuracy: 10.30%\n","  Avg Classification Loss: 0.2660\n","  Avg Reconstruction Loss: 231631.0410\n","\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 123/123 [00:29<00:00,  4.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: sketch\n","  Accuracy: 42.10%\n","  Avg Classification Loss: 0.0456\n","  Avg Reconstruction Loss: 360168.9527\n","\n"]}],"source":["# Main training and evaluation script\n","DATA_PATH = \"/kaggle/input/pacs-dataset/kfold\"  # Update this path to your dataset location\n","latent_dim = 256\n","num_classes = 7  # Update this according to your PACS dataset\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Domains in PACS dataset\n","domains = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n","\n","# Initialize models outside the loop\n","encoder = Encoder(latent_dim).to(device)\n","decoders = {domain: Decoder(latent_dim, len(domains)).to(device) for domain in domains}\n","classifier = Classifier(latent_dim, num_classes).to(device)\n","\n","# Optimizer\n","params = list(encoder.parameters()) + list(classifier.parameters())\n","for decoder in decoders.values():\n","    params += list(decoder.parameters())\n","optimizer = optim.Adam(params, lr=1e-4)\n","\n","for source_domain in domains:\n","    print(f\"\\nTraining on source domain: {source_domain}\\n\")\n","\n","    target_domains = [d for d in domains if d != source_domain]\n","\n","    # Dataloader for source domain\n","    dataloader = get_dataloader(DATA_PATH, source_domain)\n","\n","    # Train model\n","    train_model(\n","        encoder,\n","        decoders,\n","        classifier,\n","        source_domain,\n","        target_domains,\n","        dataloader,\n","        optimizer,\n","        num_epochs=10,\n","        device=device,\n","    )\n","\n","    # Evaluate model\n","    for eval_domain in target_domains:\n","        eval_dataloader = get_dataloader(DATA_PATH, eval_domain)\n","        domain_label = domains.index(eval_domain)\n","        accuracy, avg_clf_loss, avg_recon_loss = evaluate_model(\n","            encoder,\n","            classifier,\n","            decoders[eval_domain],\n","            eval_dataloader,\n","            device,\n","            domain_label,\n","        )\n","        print(f\"Accuracy on target domain '{eval_domain}': {accuracy * 100:.2f}%\")\n","        print(f\"Avg Classification Loss: {avg_clf_loss:.4f}\")\n","        print(f\"Avg Reconstruction Loss: {avg_recon_loss:.4f}\")\n","\n","# Final evaluation on all domains\n","evaluate_on_all_domains(encoder, classifier, decoders, domains, DATA_PATH, device)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-04T10:28:47.900520Z","iopub.status.busy":"2024-10-04T10:28:47.900087Z","iopub.status.idle":"2024-10-04T10:30:18.525745Z","shell.execute_reply":"2024-10-04T10:30:18.524701Z","shell.execute_reply.started":"2024-10-04T10:28:47.900479Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Final Evaluation on All Domains\n","\n","\n","Evaluting on domain art_painting\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 64/64 [00:22<00:00,  2.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: art_painting\n","  Accuracy: 10.55%\n","  Avg Classification Loss: 0.2622\n","  Avg Reconstruction Loss: 230154.9268\n","\n","\n","Evaluting on domain cartoon\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 74/74 [00:21<00:00,  3.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: cartoon\n","  Accuracy: 20.86%\n","  Avg Classification Loss: 0.1150\n","  Avg Reconstruction Loss: 252531.8223\n","\n","\n","Evaluting on domain photo\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 53/53 [00:17<00:00,  3.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Domain: photo\n","  Accuracy: 12.34%\n","  Avg Classification Loss: 0.2696\n","  Avg Reconstruction Loss: 300694.8666\n","\n","\n","Evaluting on domain sketch\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 123/123 [00:30<00:00,  4.07it/s]"]},{"name":"stdout","output_type":"stream","text":["Domain: sketch\n","  Accuracy: 42.86%\n","  Avg Classification Loss: 0.0457\n","  Avg Reconstruction Loss: 362928.5531\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Final evaluation on all domains\n","evaluate_on_all_domains(encoder, classifier, decoders, domains, DATA_PATH, device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5815134,"sourceId":9545143,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
